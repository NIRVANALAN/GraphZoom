{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, citegrh\n",
    "\n",
    "from gcn import GCN\n",
    "#from gcn_mp import GCN\n",
    "#from gcn_spmv import GCN\n",
    "# from graphsage_utils import *\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "def load_data_dgl(dataset='cora'):\n",
    "    if dataset == 'cora':\n",
    "        return citegrh.load_cora()\n",
    "    elif dataset == 'citeseer':\n",
    "        return citegrh.load_citeseer()\n",
    "    elif dataset == 'pubmed':\n",
    "        return citegrh.load_pubmed()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix, normalize=True, load_walks=False):\n",
    "    G_data = json.load(open(prefix + \"-G.json\"))\n",
    "    G = json_graph.node_link_graph(G_data)\n",
    "    if isinstance(G.nodes()[0], int):\n",
    "        def conversion(n): return int(n)\n",
    "    else:\n",
    "        def conversion(n): return n\n",
    "\n",
    "    if os.path.exists(prefix + \"-feats.npy\"):\n",
    "        feats = np.load(prefix + \"-feats.npy\")\n",
    "    else:\n",
    "        print(\"No features present.. Only identity features will be used.\")\n",
    "        feats = None\n",
    "    class_map = json.load(open(prefix + \"-class_map.json\"))\n",
    "    if isinstance(list(class_map.values())[0], list):\n",
    "        def lab_conversion(n): return n\n",
    "    else:\n",
    "        def lab_conversion(n): return int(n)\n",
    "\n",
    "    class_map = {conversion(k): lab_conversion(v)\n",
    "                 for k, v in class_map.items()}\n",
    "\n",
    "    # Remove all nodes that do not have val/test annotations\n",
    "    # (necessary because of networkx weirdness with the Reddit data)\n",
    "    broken_count = 0\n",
    "    for node in G.nodes():\n",
    "        if not 'val' in G.node[node] or not 'test' in G.node[node]:\n",
    "            G.remove_node(node)\n",
    "            broken_count += 1\n",
    "    print(\"Removed {:d} nodes that lacked proper annotations due to networkx versioning issues\".format(\n",
    "        broken_count))\n",
    "\n",
    "    # Make sure the graph has edge train_removed annotations\n",
    "    # (some datasets might already have this..)\n",
    "    print(\"Loaded data.. now preprocessing..\")\n",
    "    for edge in G.edges():\n",
    "        if (G.node[edge[0]]['val'] or G.node[edge[1]]['val'] or\n",
    "                G.node[edge[0]]['test'] or G.node[edge[1]]['test']):\n",
    "            G[edge[0]][edge[1]]['train_removed'] = True\n",
    "        else:\n",
    "            G[edge[0]][edge[1]]['train_removed'] = False\n",
    "\n",
    "    if normalize and not feats is None:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        train_ids = np.array([n for n in G.nodes(\n",
    "        ) if not G.node[n]['val'] and not G.node[n]['test']])\n",
    "        train_feats = feats[train_ids]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_feats)\n",
    "        feats = scaler.transform(feats)\n",
    "\n",
    "    return G, feats, class_map\n",
    "\n",
    "def _sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "1000\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "# train_prefix = '../graphzoom/dataset/cora/cora'\n",
    "# G, features, class_map = load_data(train_prefix)\n",
    "dataset = 'cora'\n",
    "dataset_dir = f'../graphzoom/dataset/{dataset}'\n",
    "G      = json_graph.node_link_graph(json.load(open(dataset_dir + \"/{}-G.json\".format(dataset))))\n",
    "labels = json.load(open(dataset_dir + \"/{}-class_map.json\".format(dataset)))\n",
    "feats = np.load(dataset_dir + f\"/{dataset}-feats.npy\")\n",
    "\n",
    "train_ids    = [n for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']]\n",
    "test_ids     = [n for n in G.nodes() if G.node[n]['test']]\n",
    "val_ids     = test_ids[1000:1500]\n",
    "test_ids     = test_ids[:1000]\n",
    "# train_labels = [labels[str(i)] for i in train_ids]\n",
    "# test_labels  = [labels[str(i)] for i in test_ids]\n",
    "labels = torch.LongTensor(list(class_map.values()))\n",
    "train_mask = _sample_mask(train_ids, labels.shape[0])\n",
    "test_mask =  _sample_mask(test_ids, labels.shape[0])\n",
    "val_mask =  _sample_mask(val_ids, labels.shape[0])\n",
    "# val_mask = _sample_mask(range(200, 500), labels.shape[0])\n",
    "onehot_labels = F.one_hot(labels)\n",
    "print(len(train_labels))\n",
    "print(len(test_ids))\n",
    "print(len(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7ff06b03cdd0>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EasyDict({\n",
    "    'graph': G,\n",
    "    'labels': labels,\n",
    "    'onehot_labels': onehot_labels,\n",
    "    'features': feats,\n",
    "    'train_mask':train_mask,\n",
    "    'val_mask': val_mask,\n",
    "    'test_mask': test_mask,\n",
    "    'num_classes': onehot_labels.shape[1],\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def construct_proj_laplacian(laplacian, levels, proj_dir):\n",
    "    coarse_laplacian = []\n",
    "    projections = []\n",
    "    for i in range(levels):\n",
    "        projection_name = \"{}/Projection_{}.mtx\".format(proj_dir, i+1)\n",
    "        projection = mtx2matrix(projection_name)\n",
    "        projections.append(projection)\n",
    "        coarse_laplacian.append(laplacian)\n",
    "        if i != (levels-1):\n",
    "            laplacian = projection @ laplacian @ (projection.transpose())\n",
    "    return projections, coarse_laplacian\n",
    "\n",
    "def mtx2matrix(proj_name):\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    with open(proj_name) as ff:\n",
    "        for i, line in enumerate(ff):\n",
    "            info = line.split()\n",
    "            if i == 0:\n",
    "                NumReducedNodes = int(info[0])\n",
    "                NumOriginNodes = int(info[1])\n",
    "            else:\n",
    "                row.append(int(info[0])-1)\n",
    "                col.append(int(info[1])-1)\n",
    "                data.append(1)\n",
    "    matrix = csr_matrix((data, (row, col)), shape=(\n",
    "        NumReducedNodes, NumOriginNodes))\n",
    "    return matrix\n",
    "levels = 2\n",
    "reduce_results = f\"../graphzoom/reduction_results/Cora\"\n",
    "original_adj = nx.adj_matrix(G)\n",
    "projections, coarse_adj = construct_proj_laplacian(\n",
    "    original_adj, levels, reduce_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1169, 1169)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 2., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1147, 0.1147, 0.1147,  ..., 0.1147, 0.1147, 0.3118],\n",
       "        [0.0747, 0.0747, 0.0747,  ..., 0.0747, 0.5519, 0.0747],\n",
       "        [0.1147, 0.1147, 0.1147,  ..., 0.1147, 0.1147, 0.1147],\n",
       "        ...,\n",
       "        [0.0747, 0.0747, 0.0747,  ..., 0.0747, 0.5519, 0.0747],\n",
       "        [0.0383, 0.0383, 0.0383,  ..., 0.0383, 0.0383, 0.7700],\n",
       "        [0.5519, 0.0747, 0.0747,  ..., 0.0747, 0.0747, 0.0747]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "# softmax(labels)\n",
    "coarse_feats = projections[0] @ data.features\n",
    "coarse_labels = projections[0] @ data.onehot_labels \n",
    "coarse_graph = nx.Graph(coarse_adj[1])\n",
    "rows_sum = coarse_labels.sum(axis=1)[:, np.newaxis]\n",
    "norm_coarse_labels = coarse_labels / rows_sum\n",
    "# list(map(np.shape, [coarse_embed, coarse_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_train_mask = _sample_mask(range(100), norm_coarse_labels.shape[0])\n",
    "coarse_test_mask = _sample_mask(range(100,700), norm_coarse_labels.shape[0])\n",
    "coarse_val_mask = _sample_mask(range(700,1000), norm_coarse_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_data = EasyDict({\n",
    "    'graph': coarse_graph,\n",
    "    'labels': coarse_labels,\n",
    "#     'onehot_labels': onehot_labels,\n",
    "    'features': coarse_feats,\n",
    "    'train_mask':coarse_train_mask,\n",
    "    'val_mask': coarse_val_mask,\n",
    "    'test_mask': coarse_test_mask,\n",
    "    'num_classes': norm_coarse_labels.shape[1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1169,)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = coarse_data\n",
    "data.val_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "import pdb\n",
    "def main(args):\n",
    "    # load and preprocess dataset\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.FloatTensor(data.labels)\n",
    "#     g, features, class_map = load_data(train_prefix)\n",
    "#     labels = torch.LongTensor(list(class_map.values()))\n",
    "    if hasattr(torch, 'BoolTensor'):\n",
    "        train_mask = torch.BoolTensor(data.train_mask)\n",
    "        val_mask = torch.BoolTensor(data.val_mask)\n",
    "        test_mask = torch.BoolTensor(data.test_mask)\n",
    "    in_feats = data.features.shape[1]\n",
    "    n_classes = data.num_classes\n",
    "    n_edges = data.graph.number_of_edges()\n",
    "    print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "              train_mask.int().sum().item(),\n",
    "              val_mask.int().sum().item(),\n",
    "              test_mask.int().sum().item()))\n",
    "\n",
    "    if args.gpu < 0:\n",
    "        cuda = False\n",
    "    else:\n",
    "        cuda = True\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    # graph preprocess and calculate normalization factor\n",
    "    g = data.graph\n",
    "    # add self loop\n",
    "    if args.self_loop:\n",
    "        print('add self_loop')\n",
    "        g.remove_edges_from(nx.selfloop_edges(g))\n",
    "        g.add_edges_from(zip(g.nodes(), g.nodes()))\n",
    "    g = DGLGraph(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "    # normalization\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    if cuda:\n",
    "        norm = norm.cuda()\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    # create GCN model\n",
    "    model = GCN(g,\n",
    "                in_feats,\n",
    "                args.n_hidden,\n",
    "                n_classes,\n",
    "                args.n_layers,\n",
    "                F.relu,\n",
    "                args.dropout,)\n",
    "    print(model)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "#     loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "    loss_fcn = torch.nn.KLDivLoss()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(args.n_epochs):\n",
    "        model.train()\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        # forward\n",
    "        logits = log_softmax(model(features), 1)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "#         acc = evaluate(model, features, labels, val_mask)\n",
    "        acc=0\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\n",
    "              \"ETputs(KTEPS) {:.2f}\". format(epoch, np.mean(dur), loss.item(),\n",
    "                                             acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    print()\n",
    "#     acc = evaluate(model, features, labels, test_mask)\n",
    "    print(\"Test accuracy {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='cora', dropout=0.5, gpu=0, lr=0.01, n_epochs=200, n_hidden=16, n_layers=1, self_loop=True, weight_decay=0.0005)\n",
      "----Data statistics------'\n",
      "      #Edges 3782\n",
      "      #Classes 7\n",
      "      #Train samples 100\n",
      "      #Val samples 300\n",
      "      #Test samples 600\n",
      "add self_loop\n",
      "GCN(\n",
      "  (layers): ModuleList(\n",
      "    (0): GraphConv(in=1433, out=16, normalization=True, activation=<function relu at 0x7ff0c5a1e290>)\n",
      "    (1): GraphConv(in=16, out=7, normalization=True, activation=None)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Epoch 00000 | Time(s) nan | Loss 0.7513 | Accuracy 0.0000 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 0.6971 | Accuracy 0.0000 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 0.6527 | Accuracy 0.0000 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.0033 | Loss 0.6172 | Accuracy 0.0000 | ETputs(KTEPS) 1944.93\n",
      "Epoch 00004 | Time(s) 0.0033 | Loss 0.5619 | Accuracy 0.0000 | ETputs(KTEPS) 1915.42\n",
      "Epoch 00005 | Time(s) 0.0033 | Loss 0.5300 | Accuracy 0.0000 | ETputs(KTEPS) 1932.65\n",
      "Epoch 00006 | Time(s) 0.0033 | Loss 0.5103 | Accuracy 0.0000 | ETputs(KTEPS) 1946.20\n",
      "Epoch 00007 | Time(s) 0.0033 | Loss 0.4774 | Accuracy 0.0000 | ETputs(KTEPS) 1954.14\n",
      "Epoch 00008 | Time(s) 0.0033 | Loss 0.4415 | Accuracy 0.0000 | ETputs(KTEPS) 1958.92\n",
      "Epoch 00009 | Time(s) 0.0033 | Loss 0.4273 | Accuracy 0.0000 | ETputs(KTEPS) 1963.30\n",
      "Epoch 00010 | Time(s) 0.0033 | Loss 0.4330 | Accuracy 0.0000 | ETputs(KTEPS) 1963.05\n",
      "Epoch 00011 | Time(s) 0.0033 | Loss 0.4029 | Accuracy 0.0000 | ETputs(KTEPS) 1964.06\n",
      "Epoch 00012 | Time(s) 0.0032 | Loss 0.3953 | Accuracy 0.0000 | ETputs(KTEPS) 1968.27\n",
      "Epoch 00013 | Time(s) 0.0032 | Loss 0.3687 | Accuracy 0.0000 | ETputs(KTEPS) 1970.43\n",
      "Epoch 00014 | Time(s) 0.0032 | Loss 0.3784 | Accuracy 0.0000 | ETputs(KTEPS) 1971.69\n",
      "Epoch 00015 | Time(s) 0.0032 | Loss 0.3564 | Accuracy 0.0000 | ETputs(KTEPS) 1973.62\n",
      "Epoch 00016 | Time(s) 0.0032 | Loss 0.3413 | Accuracy 0.0000 | ETputs(KTEPS) 1970.44\n",
      "Epoch 00017 | Time(s) 0.0032 | Loss 0.3578 | Accuracy 0.0000 | ETputs(KTEPS) 1971.73\n",
      "Epoch 00018 | Time(s) 0.0032 | Loss 0.3376 | Accuracy 0.0000 | ETputs(KTEPS) 1968.33\n",
      "Epoch 00019 | Time(s) 0.0033 | Loss 0.3397 | Accuracy 0.0000 | ETputs(KTEPS) 1965.25\n",
      "Epoch 00020 | Time(s) 0.0033 | Loss 0.3250 | Accuracy 0.0000 | ETputs(KTEPS) 1956.92\n",
      "Epoch 00021 | Time(s) 0.0033 | Loss 0.3318 | Accuracy 0.0000 | ETputs(KTEPS) 1942.52\n",
      "Epoch 00022 | Time(s) 0.0033 | Loss 0.3161 | Accuracy 0.0000 | ETputs(KTEPS) 1937.53\n",
      "Epoch 00023 | Time(s) 0.0033 | Loss 0.3172 | Accuracy 0.0000 | ETputs(KTEPS) 1938.92\n",
      "Epoch 00024 | Time(s) 0.0033 | Loss 0.3101 | Accuracy 0.0000 | ETputs(KTEPS) 1939.19\n",
      "Epoch 00025 | Time(s) 0.0033 | Loss 0.3174 | Accuracy 0.0000 | ETputs(KTEPS) 1940.42\n",
      "Epoch 00026 | Time(s) 0.0033 | Loss 0.3066 | Accuracy 0.0000 | ETputs(KTEPS) 1941.17\n",
      "Epoch 00027 | Time(s) 0.0033 | Loss 0.3094 | Accuracy 0.0000 | ETputs(KTEPS) 1927.48\n",
      "Epoch 00028 | Time(s) 0.0033 | Loss 0.3079 | Accuracy 0.0000 | ETputs(KTEPS) 1914.90\n",
      "Epoch 00029 | Time(s) 0.0033 | Loss 0.2950 | Accuracy 0.0000 | ETputs(KTEPS) 1914.96\n",
      "Epoch 00030 | Time(s) 0.0034 | Loss 0.3042 | Accuracy 0.0000 | ETputs(KTEPS) 1907.79\n",
      "Epoch 00031 | Time(s) 0.0034 | Loss 0.3000 | Accuracy 0.0000 | ETputs(KTEPS) 1903.40\n",
      "Epoch 00032 | Time(s) 0.0034 | Loss 0.2935 | Accuracy 0.0000 | ETputs(KTEPS) 1905.10\n",
      "Epoch 00033 | Time(s) 0.0034 | Loss 0.2874 | Accuracy 0.0000 | ETputs(KTEPS) 1905.44\n",
      "Epoch 00034 | Time(s) 0.0034 | Loss 0.2798 | Accuracy 0.0000 | ETputs(KTEPS) 1908.01\n",
      "Epoch 00035 | Time(s) 0.0033 | Loss 0.2896 | Accuracy 0.0000 | ETputs(KTEPS) 1910.56\n",
      "Epoch 00036 | Time(s) 0.0033 | Loss 0.2962 | Accuracy 0.0000 | ETputs(KTEPS) 1911.67\n",
      "Epoch 00037 | Time(s) 0.0033 | Loss 0.2817 | Accuracy 0.0000 | ETputs(KTEPS) 1913.74\n",
      "Epoch 00038 | Time(s) 0.0033 | Loss 0.2803 | Accuracy 0.0000 | ETputs(KTEPS) 1914.96\n",
      "Epoch 00039 | Time(s) 0.0033 | Loss 0.2921 | Accuracy 0.0000 | ETputs(KTEPS) 1916.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-yulan/.conda/envs/dgl/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/v-yulan/.conda/envs/dgl/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00040 | Time(s) 0.0033 | Loss 0.2886 | Accuracy 0.0000 | ETputs(KTEPS) 1917.06\n",
      "Epoch 00041 | Time(s) 0.0033 | Loss 0.2888 | Accuracy 0.0000 | ETputs(KTEPS) 1914.74\n",
      "Epoch 00042 | Time(s) 0.0033 | Loss 0.2885 | Accuracy 0.0000 | ETputs(KTEPS) 1915.72\n",
      "Epoch 00043 | Time(s) 0.0033 | Loss 0.2809 | Accuracy 0.0000 | ETputs(KTEPS) 1916.94\n",
      "Epoch 00044 | Time(s) 0.0033 | Loss 0.2847 | Accuracy 0.0000 | ETputs(KTEPS) 1917.07\n",
      "Epoch 00045 | Time(s) 0.0033 | Loss 0.2783 | Accuracy 0.0000 | ETputs(KTEPS) 1918.94\n",
      "Epoch 00046 | Time(s) 0.0033 | Loss 0.2915 | Accuracy 0.0000 | ETputs(KTEPS) 1918.20\n",
      "Epoch 00047 | Time(s) 0.0033 | Loss 0.2820 | Accuracy 0.0000 | ETputs(KTEPS) 1918.39\n",
      "Epoch 00048 | Time(s) 0.0033 | Loss 0.2899 | Accuracy 0.0000 | ETputs(KTEPS) 1912.69\n",
      "Epoch 00049 | Time(s) 0.0033 | Loss 0.2825 | Accuracy 0.0000 | ETputs(KTEPS) 1912.17\n",
      "Epoch 00050 | Time(s) 0.0033 | Loss 0.2869 | Accuracy 0.0000 | ETputs(KTEPS) 1909.75\n",
      "Epoch 00051 | Time(s) 0.0033 | Loss 0.2732 | Accuracy 0.0000 | ETputs(KTEPS) 1911.18\n",
      "Epoch 00052 | Time(s) 0.0033 | Loss 0.2783 | Accuracy 0.0000 | ETputs(KTEPS) 1912.26\n",
      "Epoch 00053 | Time(s) 0.0033 | Loss 0.2813 | Accuracy 0.0000 | ETputs(KTEPS) 1912.27\n",
      "Epoch 00054 | Time(s) 0.0033 | Loss 0.2742 | Accuracy 0.0000 | ETputs(KTEPS) 1912.99\n",
      "Epoch 00055 | Time(s) 0.0033 | Loss 0.2803 | Accuracy 0.0000 | ETputs(KTEPS) 1914.04\n",
      "Epoch 00056 | Time(s) 0.0033 | Loss 0.2897 | Accuracy 0.0000 | ETputs(KTEPS) 1914.28\n",
      "Epoch 00057 | Time(s) 0.0033 | Loss 0.2922 | Accuracy 0.0000 | ETputs(KTEPS) 1911.60\n",
      "Epoch 00058 | Time(s) 0.0033 | Loss 0.2786 | Accuracy 0.0000 | ETputs(KTEPS) 1912.55\n",
      "Epoch 00059 | Time(s) 0.0033 | Loss 0.2866 | Accuracy 0.0000 | ETputs(KTEPS) 1913.59\n",
      "Epoch 00060 | Time(s) 0.0033 | Loss 0.2948 | Accuracy 0.0000 | ETputs(KTEPS) 1914.04\n",
      "Epoch 00061 | Time(s) 0.0033 | Loss 0.2919 | Accuracy 0.0000 | ETputs(KTEPS) 1914.80\n",
      "Epoch 00062 | Time(s) 0.0033 | Loss 0.2939 | Accuracy 0.0000 | ETputs(KTEPS) 1915.88\n",
      "Epoch 00063 | Time(s) 0.0033 | Loss 0.2818 | Accuracy 0.0000 | ETputs(KTEPS) 1916.80\n",
      "Epoch 00064 | Time(s) 0.0033 | Loss 0.2769 | Accuracy 0.0000 | ETputs(KTEPS) 1917.46\n",
      "Epoch 00065 | Time(s) 0.0033 | Loss 0.2797 | Accuracy 0.0000 | ETputs(KTEPS) 1917.97\n",
      "Epoch 00066 | Time(s) 0.0033 | Loss 0.2725 | Accuracy 0.0000 | ETputs(KTEPS) 1919.03\n",
      "Epoch 00067 | Time(s) 0.0033 | Loss 0.2739 | Accuracy 0.0000 | ETputs(KTEPS) 1918.55\n",
      "Epoch 00068 | Time(s) 0.0033 | Loss 0.2867 | Accuracy 0.0000 | ETputs(KTEPS) 1919.54\n",
      "Epoch 00069 | Time(s) 0.0033 | Loss 0.2763 | Accuracy 0.0000 | ETputs(KTEPS) 1920.08\n",
      "Epoch 00070 | Time(s) 0.0033 | Loss 0.2692 | Accuracy 0.0000 | ETputs(KTEPS) 1919.91\n",
      "Epoch 00071 | Time(s) 0.0033 | Loss 0.2702 | Accuracy 0.0000 | ETputs(KTEPS) 1920.65\n",
      "Epoch 00072 | Time(s) 0.0033 | Loss 0.2834 | Accuracy 0.0000 | ETputs(KTEPS) 1921.74\n",
      "Epoch 00073 | Time(s) 0.0033 | Loss 0.2827 | Accuracy 0.0000 | ETputs(KTEPS) 1922.49\n",
      "Epoch 00074 | Time(s) 0.0033 | Loss 0.2832 | Accuracy 0.0000 | ETputs(KTEPS) 1923.14\n",
      "Epoch 00075 | Time(s) 0.0033 | Loss 0.2887 | Accuracy 0.0000 | ETputs(KTEPS) 1922.82\n",
      "Epoch 00076 | Time(s) 0.0033 | Loss 0.2754 | Accuracy 0.0000 | ETputs(KTEPS) 1923.44\n",
      "Epoch 00077 | Time(s) 0.0033 | Loss 0.2791 | Accuracy 0.0000 | ETputs(KTEPS) 1924.97\n",
      "Epoch 00078 | Time(s) 0.0033 | Loss 0.2764 | Accuracy 0.0000 | ETputs(KTEPS) 1925.29\n",
      "Epoch 00079 | Time(s) 0.0033 | Loss 0.2892 | Accuracy 0.0000 | ETputs(KTEPS) 1925.91\n",
      "Epoch 00080 | Time(s) 0.0033 | Loss 0.2753 | Accuracy 0.0000 | ETputs(KTEPS) 1925.87\n",
      "Epoch 00081 | Time(s) 0.0033 | Loss 0.2829 | Accuracy 0.0000 | ETputs(KTEPS) 1926.14\n",
      "Epoch 00082 | Time(s) 0.0033 | Loss 0.2850 | Accuracy 0.0000 | ETputs(KTEPS) 1926.64\n",
      "Epoch 00083 | Time(s) 0.0033 | Loss 0.2706 | Accuracy 0.0000 | ETputs(KTEPS) 1927.25\n",
      "Epoch 00084 | Time(s) 0.0033 | Loss 0.2839 | Accuracy 0.0000 | ETputs(KTEPS) 1927.99\n",
      "Epoch 00085 | Time(s) 0.0033 | Loss 0.2706 | Accuracy 0.0000 | ETputs(KTEPS) 1928.82\n",
      "Epoch 00086 | Time(s) 0.0033 | Loss 0.2772 | Accuracy 0.0000 | ETputs(KTEPS) 1929.61\n",
      "Epoch 00087 | Time(s) 0.0033 | Loss 0.2777 | Accuracy 0.0000 | ETputs(KTEPS) 1930.49\n",
      "Epoch 00088 | Time(s) 0.0033 | Loss 0.2828 | Accuracy 0.0000 | ETputs(KTEPS) 1930.85\n",
      "Epoch 00089 | Time(s) 0.0033 | Loss 0.2822 | Accuracy 0.0000 | ETputs(KTEPS) 1931.40\n",
      "Epoch 00090 | Time(s) 0.0033 | Loss 0.2647 | Accuracy 0.0000 | ETputs(KTEPS) 1932.39\n",
      "Epoch 00091 | Time(s) 0.0033 | Loss 0.2725 | Accuracy 0.0000 | ETputs(KTEPS) 1933.29\n",
      "Epoch 00092 | Time(s) 0.0033 | Loss 0.2845 | Accuracy 0.0000 | ETputs(KTEPS) 1934.07\n",
      "Epoch 00093 | Time(s) 0.0033 | Loss 0.2767 | Accuracy 0.0000 | ETputs(KTEPS) 1934.98\n",
      "Epoch 00094 | Time(s) 0.0033 | Loss 0.2753 | Accuracy 0.0000 | ETputs(KTEPS) 1935.58\n",
      "Epoch 00095 | Time(s) 0.0033 | Loss 0.2822 | Accuracy 0.0000 | ETputs(KTEPS) 1936.31\n",
      "Epoch 00096 | Time(s) 0.0033 | Loss 0.2772 | Accuracy 0.0000 | ETputs(KTEPS) 1937.26\n",
      "Epoch 00097 | Time(s) 0.0033 | Loss 0.2731 | Accuracy 0.0000 | ETputs(KTEPS) 1937.92\n",
      "Epoch 00098 | Time(s) 0.0033 | Loss 0.2725 | Accuracy 0.0000 | ETputs(KTEPS) 1938.18\n",
      "Epoch 00099 | Time(s) 0.0033 | Loss 0.2747 | Accuracy 0.0000 | ETputs(KTEPS) 1936.21\n",
      "Epoch 00100 | Time(s) 0.0033 | Loss 0.2725 | Accuracy 0.0000 | ETputs(KTEPS) 1936.59\n",
      "Epoch 00101 | Time(s) 0.0033 | Loss 0.2767 | Accuracy 0.0000 | ETputs(KTEPS) 1936.95\n",
      "Epoch 00102 | Time(s) 0.0033 | Loss 0.2833 | Accuracy 0.0000 | ETputs(KTEPS) 1936.42\n",
      "Epoch 00103 | Time(s) 0.0033 | Loss 0.2839 | Accuracy 0.0000 | ETputs(KTEPS) 1936.79\n",
      "Epoch 00104 | Time(s) 0.0033 | Loss 0.2841 | Accuracy 0.0000 | ETputs(KTEPS) 1937.66\n",
      "Epoch 00105 | Time(s) 0.0033 | Loss 0.2808 | Accuracy 0.0000 | ETputs(KTEPS) 1938.37\n",
      "Epoch 00106 | Time(s) 0.0033 | Loss 0.2782 | Accuracy 0.0000 | ETputs(KTEPS) 1939.15\n",
      "Epoch 00107 | Time(s) 0.0033 | Loss 0.2662 | Accuracy 0.0000 | ETputs(KTEPS) 1939.86\n",
      "Epoch 00108 | Time(s) 0.0033 | Loss 0.2814 | Accuracy 0.0000 | ETputs(KTEPS) 1940.61\n",
      "Epoch 00109 | Time(s) 0.0033 | Loss 0.2679 | Accuracy 0.0000 | ETputs(KTEPS) 1941.31\n",
      "Epoch 00110 | Time(s) 0.0033 | Loss 0.2816 | Accuracy 0.0000 | ETputs(KTEPS) 1942.13\n",
      "Epoch 00111 | Time(s) 0.0033 | Loss 0.2725 | Accuracy 0.0000 | ETputs(KTEPS) 1942.76\n",
      "Epoch 00112 | Time(s) 0.0033 | Loss 0.2792 | Accuracy 0.0000 | ETputs(KTEPS) 1943.22\n",
      "Epoch 00113 | Time(s) 0.0033 | Loss 0.2725 | Accuracy 0.0000 | ETputs(KTEPS) 1943.63\n",
      "Epoch 00114 | Time(s) 0.0033 | Loss 0.2732 | Accuracy 0.0000 | ETputs(KTEPS) 1944.21\n",
      "Epoch 00115 | Time(s) 0.0033 | Loss 0.2647 | Accuracy 0.0000 | ETputs(KTEPS) 1944.87\n",
      "Epoch 00116 | Time(s) 0.0033 | Loss 0.2746 | Accuracy 0.0000 | ETputs(KTEPS) 1945.62\n",
      "Epoch 00117 | Time(s) 0.0033 | Loss 0.2714 | Accuracy 0.0000 | ETputs(KTEPS) 1946.50\n",
      "Epoch 00118 | Time(s) 0.0033 | Loss 0.2879 | Accuracy 0.0000 | ETputs(KTEPS) 1947.05\n",
      "Epoch 00119 | Time(s) 0.0033 | Loss 0.2688 | Accuracy 0.0000 | ETputs(KTEPS) 1947.35\n",
      "Epoch 00120 | Time(s) 0.0033 | Loss 0.2730 | Accuracy 0.0000 | ETputs(KTEPS) 1947.47\n",
      "Epoch 00121 | Time(s) 0.0033 | Loss 0.2723 | Accuracy 0.0000 | ETputs(KTEPS) 1947.81\n",
      "Epoch 00122 | Time(s) 0.0033 | Loss 0.2828 | Accuracy 0.0000 | ETputs(KTEPS) 1948.05\n",
      "Epoch 00123 | Time(s) 0.0033 | Loss 0.2750 | Accuracy 0.0000 | ETputs(KTEPS) 1948.74\n",
      "Epoch 00124 | Time(s) 0.0033 | Loss 0.2653 | Accuracy 0.0000 | ETputs(KTEPS) 1949.07\n",
      "Epoch 00125 | Time(s) 0.0033 | Loss 0.2780 | Accuracy 0.0000 | ETputs(KTEPS) 1949.37\n",
      "Epoch 00126 | Time(s) 0.0033 | Loss 0.2674 | Accuracy 0.0000 | ETputs(KTEPS) 1949.75\n",
      "Epoch 00127 | Time(s) 0.0033 | Loss 0.2700 | Accuracy 0.0000 | ETputs(KTEPS) 1949.12\n",
      "Epoch 00128 | Time(s) 0.0033 | Loss 0.2796 | Accuracy 0.0000 | ETputs(KTEPS) 1949.55\n",
      "Epoch 00129 | Time(s) 0.0033 | Loss 0.2781 | Accuracy 0.0000 | ETputs(KTEPS) 1949.80\n",
      "Epoch 00130 | Time(s) 0.0033 | Loss 0.2618 | Accuracy 0.0000 | ETputs(KTEPS) 1950.31\n",
      "Epoch 00131 | Time(s) 0.0033 | Loss 0.2818 | Accuracy 0.0000 | ETputs(KTEPS) 1950.65\n",
      "Epoch 00132 | Time(s) 0.0033 | Loss 0.2669 | Accuracy 0.0000 | ETputs(KTEPS) 1951.08\n",
      "Epoch 00133 | Time(s) 0.0033 | Loss 0.2636 | Accuracy 0.0000 | ETputs(KTEPS) 1951.51\n",
      "Epoch 00134 | Time(s) 0.0033 | Loss 0.2724 | Accuracy 0.0000 | ETputs(KTEPS) 1951.97\n",
      "Epoch 00135 | Time(s) 0.0033 | Loss 0.2881 | Accuracy 0.0000 | ETputs(KTEPS) 1952.30\n",
      "Epoch 00136 | Time(s) 0.0033 | Loss 0.2777 | Accuracy 0.0000 | ETputs(KTEPS) 1952.91\n",
      "Epoch 00137 | Time(s) 0.0033 | Loss 0.2791 | Accuracy 0.0000 | ETputs(KTEPS) 1953.66\n",
      "Epoch 00138 | Time(s) 0.0033 | Loss 0.2872 | Accuracy 0.0000 | ETputs(KTEPS) 1953.54\n",
      "Epoch 00139 | Time(s) 0.0033 | Loss 0.2711 | Accuracy 0.0000 | ETputs(KTEPS) 1953.87\n",
      "Epoch 00140 | Time(s) 0.0033 | Loss 0.2730 | Accuracy 0.0000 | ETputs(KTEPS) 1954.21\n",
      "Epoch 00141 | Time(s) 0.0033 | Loss 0.2789 | Accuracy 0.0000 | ETputs(KTEPS) 1954.45\n",
      "Epoch 00142 | Time(s) 0.0033 | Loss 0.2641 | Accuracy 0.0000 | ETputs(KTEPS) 1954.77\n",
      "Epoch 00143 | Time(s) 0.0033 | Loss 0.2666 | Accuracy 0.0000 | ETputs(KTEPS) 1955.29\n",
      "Epoch 00144 | Time(s) 0.0033 | Loss 0.2787 | Accuracy 0.0000 | ETputs(KTEPS) 1955.40\n",
      "Epoch 00145 | Time(s) 0.0033 | Loss 0.2730 | Accuracy 0.0000 | ETputs(KTEPS) 1955.81\n",
      "Epoch 00146 | Time(s) 0.0033 | Loss 0.2650 | Accuracy 0.0000 | ETputs(KTEPS) 1956.30\n",
      "Epoch 00147 | Time(s) 0.0033 | Loss 0.2703 | Accuracy 0.0000 | ETputs(KTEPS) 1956.79\n",
      "Epoch 00148 | Time(s) 0.0033 | Loss 0.2687 | Accuracy 0.0000 | ETputs(KTEPS) 1957.31\n",
      "Epoch 00149 | Time(s) 0.0033 | Loss 0.2714 | Accuracy 0.0000 | ETputs(KTEPS) 1957.46\n",
      "Epoch 00150 | Time(s) 0.0033 | Loss 0.2717 | Accuracy 0.0000 | ETputs(KTEPS) 1957.91\n",
      "Epoch 00151 | Time(s) 0.0033 | Loss 0.2764 | Accuracy 0.0000 | ETputs(KTEPS) 1958.23\n",
      "Epoch 00152 | Time(s) 0.0033 | Loss 0.2590 | Accuracy 0.0000 | ETputs(KTEPS) 1958.33\n",
      "Epoch 00153 | Time(s) 0.0033 | Loss 0.2587 | Accuracy 0.0000 | ETputs(KTEPS) 1958.71\n",
      "Epoch 00154 | Time(s) 0.0033 | Loss 0.2687 | Accuracy 0.0000 | ETputs(KTEPS) 1959.15\n",
      "Epoch 00155 | Time(s) 0.0033 | Loss 0.2681 | Accuracy 0.0000 | ETputs(KTEPS) 1959.51\n",
      "Epoch 00156 | Time(s) 0.0033 | Loss 0.2722 | Accuracy 0.0000 | ETputs(KTEPS) 1959.80\n",
      "Epoch 00157 | Time(s) 0.0033 | Loss 0.2664 | Accuracy 0.0000 | ETputs(KTEPS) 1959.66\n",
      "Epoch 00158 | Time(s) 0.0033 | Loss 0.2655 | Accuracy 0.0000 | ETputs(KTEPS) 1957.57\n",
      "Epoch 00159 | Time(s) 0.0033 | Loss 0.2667 | Accuracy 0.0000 | ETputs(KTEPS) 1957.49\n",
      "Epoch 00160 | Time(s) 0.0033 | Loss 0.2770 | Accuracy 0.0000 | ETputs(KTEPS) 1957.69\n",
      "Epoch 00161 | Time(s) 0.0033 | Loss 0.2619 | Accuracy 0.0000 | ETputs(KTEPS) 1958.03\n",
      "Epoch 00162 | Time(s) 0.0033 | Loss 0.2624 | Accuracy 0.0000 | ETputs(KTEPS) 1958.36\n",
      "Epoch 00163 | Time(s) 0.0033 | Loss 0.2666 | Accuracy 0.0000 | ETputs(KTEPS) 1958.74\n",
      "Epoch 00164 | Time(s) 0.0033 | Loss 0.2684 | Accuracy 0.0000 | ETputs(KTEPS) 1959.23\n",
      "Epoch 00165 | Time(s) 0.0033 | Loss 0.2645 | Accuracy 0.0000 | ETputs(KTEPS) 1959.57\n",
      "Epoch 00166 | Time(s) 0.0033 | Loss 0.2662 | Accuracy 0.0000 | ETputs(KTEPS) 1959.90\n",
      "Epoch 00167 | Time(s) 0.0033 | Loss 0.2687 | Accuracy 0.0000 | ETputs(KTEPS) 1960.07\n",
      "Epoch 00168 | Time(s) 0.0033 | Loss 0.2700 | Accuracy 0.0000 | ETputs(KTEPS) 1959.98\n",
      "Epoch 00169 | Time(s) 0.0033 | Loss 0.2734 | Accuracy 0.0000 | ETputs(KTEPS) 1960.21\n",
      "Epoch 00170 | Time(s) 0.0033 | Loss 0.2637 | Accuracy 0.0000 | ETputs(KTEPS) 1960.55\n",
      "Epoch 00171 | Time(s) 0.0033 | Loss 0.2700 | Accuracy 0.0000 | ETputs(KTEPS) 1960.77\n",
      "Epoch 00172 | Time(s) 0.0033 | Loss 0.2750 | Accuracy 0.0000 | ETputs(KTEPS) 1961.07\n",
      "Epoch 00173 | Time(s) 0.0033 | Loss 0.2663 | Accuracy 0.0000 | ETputs(KTEPS) 1960.62\n",
      "Epoch 00174 | Time(s) 0.0033 | Loss 0.2643 | Accuracy 0.0000 | ETputs(KTEPS) 1958.61\n",
      "Epoch 00175 | Time(s) 0.0033 | Loss 0.2797 | Accuracy 0.0000 | ETputs(KTEPS) 1958.87\n",
      "Epoch 00176 | Time(s) 0.0033 | Loss 0.2617 | Accuracy 0.0000 | ETputs(KTEPS) 1958.31\n",
      "Epoch 00177 | Time(s) 0.0033 | Loss 0.2618 | Accuracy 0.0000 | ETputs(KTEPS) 1957.52\n",
      "Epoch 00178 | Time(s) 0.0033 | Loss 0.2675 | Accuracy 0.0000 | ETputs(KTEPS) 1957.85\n",
      "Epoch 00179 | Time(s) 0.0033 | Loss 0.2701 | Accuracy 0.0000 | ETputs(KTEPS) 1958.18\n",
      "Epoch 00180 | Time(s) 0.0033 | Loss 0.2609 | Accuracy 0.0000 | ETputs(KTEPS) 1958.50\n",
      "Epoch 00181 | Time(s) 0.0033 | Loss 0.2668 | Accuracy 0.0000 | ETputs(KTEPS) 1958.80\n",
      "Epoch 00182 | Time(s) 0.0033 | Loss 0.2742 | Accuracy 0.0000 | ETputs(KTEPS) 1959.10\n",
      "Epoch 00183 | Time(s) 0.0033 | Loss 0.2682 | Accuracy 0.0000 | ETputs(KTEPS) 1959.49\n",
      "Epoch 00184 | Time(s) 0.0033 | Loss 0.2720 | Accuracy 0.0000 | ETputs(KTEPS) 1959.77\n",
      "Epoch 00185 | Time(s) 0.0033 | Loss 0.2685 | Accuracy 0.0000 | ETputs(KTEPS) 1960.17\n",
      "Epoch 00186 | Time(s) 0.0033 | Loss 0.2692 | Accuracy 0.0000 | ETputs(KTEPS) 1960.52\n",
      "Epoch 00187 | Time(s) 0.0033 | Loss 0.2673 | Accuracy 0.0000 | ETputs(KTEPS) 1960.89\n",
      "Epoch 00188 | Time(s) 0.0033 | Loss 0.2671 | Accuracy 0.0000 | ETputs(KTEPS) 1961.22\n",
      "Epoch 00189 | Time(s) 0.0033 | Loss 0.2701 | Accuracy 0.0000 | ETputs(KTEPS) 1961.58\n",
      "Epoch 00190 | Time(s) 0.0033 | Loss 0.2643 | Accuracy 0.0000 | ETputs(KTEPS) 1961.90\n",
      "Epoch 00191 | Time(s) 0.0033 | Loss 0.2713 | Accuracy 0.0000 | ETputs(KTEPS) 1962.32\n",
      "Epoch 00192 | Time(s) 0.0033 | Loss 0.2644 | Accuracy 0.0000 | ETputs(KTEPS) 1962.61\n",
      "Epoch 00193 | Time(s) 0.0033 | Loss 0.2699 | Accuracy 0.0000 | ETputs(KTEPS) 1963.06\n",
      "Epoch 00194 | Time(s) 0.0033 | Loss 0.2691 | Accuracy 0.0000 | ETputs(KTEPS) 1963.26\n",
      "Epoch 00195 | Time(s) 0.0033 | Loss 0.2727 | Accuracy 0.0000 | ETputs(KTEPS) 1963.62\n",
      "Epoch 00196 | Time(s) 0.0033 | Loss 0.2761 | Accuracy 0.0000 | ETputs(KTEPS) 1963.80\n",
      "Epoch 00197 | Time(s) 0.0033 | Loss 0.2661 | Accuracy 0.0000 | ETputs(KTEPS) 1964.03\n",
      "Epoch 00198 | Time(s) 0.0033 | Loss 0.2622 | Accuracy 0.0000 | ETputs(KTEPS) 1964.36\n",
      "Epoch 00199 | Time(s) 0.0033 | Loss 0.2681 | Accuracy 0.0000 | ETputs(KTEPS) 1964.72\n",
      "\n",
      "Test accuracy 0.00%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='GCN')\n",
    "#     register_data_args(parser)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.5,\n",
    "            help=\"dropout probability\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "            help=\"gpu\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-2,\n",
    "            help=\"learning rate\")\n",
    "    parser.add_argument(\"--n-epochs\", type=int, default=200,\n",
    "            help=\"number of training epochs\")\n",
    "    parser.add_argument(\"--n-hidden\", type=int, default=16,\n",
    "            help=\"number of hidden gcn units\")\n",
    "    parser.add_argument(\"--n-layers\", type=int, default=1,\n",
    "            help=\"number of hidden gcn layers\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=5e-4,\n",
    "            help=\"Weight for L2 loss\")\n",
    "    parser.add_argument(\"--self-loop\", action='store_true',\n",
    "            help=\"graph self-loop (default=False)\")\n",
    "#     parser.add_argument(\"--dataset\", default='cora')\n",
    "    parser.set_defaults(self_loop=True)\n",
    "    parser.add_argument(\"--dataset\", default='cora')\n",
    "#     args = parser.parse_args()\n",
    "#     args = parser.parse_args()[1:]\n",
    "    args = parser.parse_known_args()[0]\n",
    "    print(args)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments\n",
    "dataset | level | acc\n",
    "--- | --- | ---\n",
    "cora | 0 | 81.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
