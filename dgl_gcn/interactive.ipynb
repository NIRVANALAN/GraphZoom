{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, citegrh\n",
    "\n",
    "# from gcn import GCN\n",
    "#from gcn_mp import GCN\n",
    "#from gcn_spmv import GCN\n",
    "# from graphsage_utils import *\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "def load_data_dgl(dataset='cora'):\n",
    "    if dataset == 'cora':\n",
    "        return citegrh.load_cora()\n",
    "    elif dataset == 'citeseer':\n",
    "        return citegrh.load_citeseer()\n",
    "    elif dataset == 'pubmed':\n",
    "        return citegrh.load_pubmed()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GCN using DGL nn package\n",
    "\n",
    "References:\n",
    "- Semi-Supervised Classification with Graph Convolutional Networks\n",
    "- Paper: https://arxiv.org/abs/1609.02907\n",
    "- Code: https://github.com/tkipf/gcn\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout, log_softmax=False):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.log_softmax = log_softmax\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            emb = h\n",
    "            h = layer(self.g, h)\n",
    "        if self.log_softmax:\n",
    "            return nn.functional.log_softmax(h, 1), emb\n",
    "        return h, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix, normalize=True, load_walks=False):\n",
    "    G_data = json.load(open(prefix + \"-G.json\"))\n",
    "    G = json_graph.node_link_graph(G_data)\n",
    "    if isinstance(G.nodes()[0], int):\n",
    "        def conversion(n): return int(n)\n",
    "    else:\n",
    "        def conversion(n): return n\n",
    "\n",
    "    if os.path.exists(prefix + \"-feats.npy\"):\n",
    "        feats = np.load(prefix + \"-feats.npy\")\n",
    "    else:\n",
    "        print(\"No features present.. Only identity features will be used.\")\n",
    "        feats = None\n",
    "    class_map = json.load(open(prefix + \"-class_map.json\"))\n",
    "    if isinstance(list(class_map.values())[0], list):\n",
    "        def lab_conversion(n): return n\n",
    "    else:\n",
    "        def lab_conversion(n): return int(n)\n",
    "\n",
    "    class_map = {conversion(k): lab_conversion(v)\n",
    "                 for k, v in class_map.items()}\n",
    "\n",
    "    # Remove all nodes that do not have val/test annotations\n",
    "    # (necessary because of networkx weirdness with the Reddit data)\n",
    "    broken_count = 0\n",
    "    for node in G.nodes():\n",
    "        if not 'val' in G.node[node] or not 'test' in G.node[node]:\n",
    "            G.remove_node(node)\n",
    "            broken_count += 1\n",
    "    print(\"Removed {:d} nodes that lacked proper annotations due to networkx versioning issues\".format(\n",
    "        broken_count))\n",
    "\n",
    "    # Make sure the graph has edge train_removed annotations\n",
    "    # (some datasets might already have this..)\n",
    "    print(\"Loaded data.. now preprocessing..\")\n",
    "    for edge in G.edges():\n",
    "        if (G.node[edge[0]]['val'] or G.node[edge[1]]['val'] or\n",
    "                G.node[edge[0]]['test'] or G.node[edge[1]]['test']):\n",
    "            G[edge[0]][edge[1]]['train_removed'] = True\n",
    "        else:\n",
    "            G[edge[0]][edge[1]]['train_removed'] = False\n",
    "\n",
    "    if normalize and not feats is None:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        train_ids = np.array([n for n in G.nodes(\n",
    "        ) if not G.node[n]['val'] and not G.node[n]['test']])\n",
    "        train_feats = feats[train_ids]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_feats)\n",
    "        feats = scaler.transform(feats)\n",
    "\n",
    "    return G, feats, class_map\n",
    "\n",
    "def _sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "1000\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "# train_prefix = '../graphzoom/dataset/cora/cora'\n",
    "# G, features, class_map = load_data(train_prefix)\n",
    "dataset = 'pubmed'\n",
    "dataset_dir = f'../graphzoom/dataset/{dataset}'\n",
    "G      = json_graph.node_link_graph(json.load(open(dataset_dir + \"/{}-G.json\".format(dataset))))\n",
    "labels = json.load(open(dataset_dir + \"/{}-class_map.json\".format(dataset)))\n",
    "feats = np.load(dataset_dir + f\"/{dataset}-feats.npy\")\n",
    "\n",
    "train_ids    = [n for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']]\n",
    "test_ids     = [n for n in G.nodes() if G.node[n]['test']]\n",
    "val_ids     = test_ids[1000:1500]\n",
    "test_ids     = test_ids[:1000]\n",
    "# train_labels = [labels[str(i)] for i in train_ids]\n",
    "# test_labels  = [labels[str(i)] for i in test_ids]\n",
    "labels = torch.LongTensor(list(labels.values()))\n",
    "train_mask = _sample_mask(train_ids, labels.shape[0])\n",
    "test_mask =  _sample_mask(test_ids, labels.shape[0])\n",
    "val_mask =  _sample_mask(val_ids, labels.shape[0])\n",
    "# val_mask = _sample_mask(range(200, 500), labels.shape[0])\n",
    "onehot_labels = F.one_hot(labels)\n",
    "print(len(train_labels))\n",
    "print(len(test_ids))\n",
    "print(len(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EasyDict({\n",
    "    'graph': G,\n",
    "    'labels': labels,\n",
    "    'onehot_labels': onehot_labels,\n",
    "    'features': feats,\n",
    "    'train_mask':train_mask,\n",
    "    'val_mask': val_mask,\n",
    "    'test_mask': test_mask,\n",
    "    'num_classes': onehot_labels.shape[1],\n",
    "    'coarse': False\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def construct_proj_laplacian(laplacian, levels, proj_dir):\n",
    "    coarse_laplacian = []\n",
    "    projections = []\n",
    "    for i in range(levels):\n",
    "        projection_name = \"{}/Projection_{}.mtx\".format(proj_dir, i+1)\n",
    "        projection = mtx2matrix(projection_name)\n",
    "        projections.append(projection)\n",
    "        coarse_laplacian.append(laplacian)\n",
    "        if i != (levels-1):\n",
    "            laplacian = projection @ laplacian @ (projection.transpose())\n",
    "    return projections, coarse_laplacian\n",
    "\n",
    "def mtx2matrix(proj_name):\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    with open(proj_name) as ff:\n",
    "        for i, line in enumerate(ff):\n",
    "            info = line.split()\n",
    "            if i == 0:\n",
    "                NumReducedNodes = int(info[0])\n",
    "                NumOriginNodes = int(info[1])\n",
    "            else:\n",
    "                row.append(int(info[0])-1)\n",
    "                col.append(int(info[1])-1)\n",
    "                data.append(1)\n",
    "    matrix = csr_matrix((data, (row, col)), shape=(\n",
    "        NumReducedNodes, NumOriginNodes))\n",
    "    return matrix\n",
    "levels = 2\n",
    "reduce_results = f\"../graphzoom/reduction_results/{dataset}\"\n",
    "original_adj = nx.adj_matrix(G)\n",
    "projections, coarse_adj = construct_proj_laplacian(\n",
    "    original_adj, levels, reduce_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "# softmax(labels)\n",
    "# level = 1\n",
    "coarse_feats = projections[0] @ data.features\n",
    "coarse_labels = projections[0] @ data.onehot_labels \n",
    "coarse_graph = nx.Graph(coarse_adj[1])\n",
    "rows_sum = coarse_labels.sum(axis=1)[:, np.newaxis]\n",
    "norm_coarse_labels = coarse_labels / rows_sum\n",
    "# list(map(np.shape, [coarse_embed, coarse_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_coarse_labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_train_mask = _sample_mask(range(100), norm_coarse_labels.shape[0])\n",
    "coarse_test_mask = _sample_mask(range(100,700), norm_coarse_labels.shape[0])\n",
    "coarse_val_mask = _sample_mask(range(700,1000), norm_coarse_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_data = EasyDict({\n",
    "    'graph': coarse_graph,\n",
    "    'labels': coarse_labels,\n",
    "#     'onehot_labels': onehot_labels,\n",
    "    'features': coarse_feats,\n",
    "    'train_mask':coarse_train_mask,\n",
    "    'val_mask': coarse_val_mask,\n",
    "    'test_mask': coarse_test_mask,\n",
    "    'num_classes': norm_coarse_labels.shape[1],\n",
    "    'coarse' : True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7903, 500)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = coarse_data\n",
    "data.val_mask.shape\n",
    "data.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "import pdb\n",
    "def main(args):\n",
    "    # load and preprocess dataset\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    if data.coarse:\n",
    "        labels = torch.FloatTensor(data.labels)\n",
    "        loss_fcn = torch.nn.KLDivLoss()\n",
    "    else:\n",
    "        labels = torch.LongTensor(data.labels)\n",
    "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "#     g, features, class_map = load_data(train_prefix)\n",
    "#     labels = torch.LongTensor(list(class_map.values()))\n",
    "    if hasattr(torch, 'BoolTensor'):\n",
    "        train_mask = torch.BoolTensor(data.train_mask)\n",
    "        val_mask = torch.BoolTensor(data.val_mask)\n",
    "        test_mask = torch.BoolTensor(data.test_mask)\n",
    "    in_feats = data.features.shape[1]\n",
    "    n_classes = data.num_classes\n",
    "    n_edges = data.graph.number_of_edges()\n",
    "    print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "              train_mask.int().sum().item(),\n",
    "              val_mask.int().sum().item(),\n",
    "              test_mask.int().sum().item()))\n",
    "\n",
    "    if args.gpu < 0:\n",
    "        cuda = False\n",
    "    else:\n",
    "        cuda = True\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    # graph preprocess and calculate normalization factor\n",
    "    g = data.graph\n",
    "    # add self loop\n",
    "    if args.self_loop:\n",
    "        print('add self_loop')\n",
    "        g.remove_edges_from(nx.selfloop_edges(g))\n",
    "        g.add_edges_from(zip(g.nodes(), g.nodes()))\n",
    "    g = DGLGraph(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "    # normalization\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    if cuda:\n",
    "        norm = norm.cuda()\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    # create GCN model\n",
    "    model = GCN(g,\n",
    "                in_feats,\n",
    "                args.n_hidden,\n",
    "                n_classes,\n",
    "                args.n_layers,\n",
    "                F.relu,\n",
    "                args.dropout,log_softmax=data.coarse)\n",
    "    print(model)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(args.n_epochs):\n",
    "        model.train()\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        # forward\n",
    "        logits, h = model(features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "#         acc = evaluate(model, features, labels, val_mask)\n",
    "        acc=0\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\n",
    "              \"ETputs(KTEPS) {:.2f}\". format(epoch, np.mean(dur), loss.item(),\n",
    "                                             acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    print()\n",
    "    print(h.shape)\n",
    "    np.save(f'{dataset}_emb_level_1', h.detach().cpu().numpy())\n",
    "#     acc = evaluate(model, features, labels, test_mask)\n",
    "    print(\"Test accuracy {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='cora', dropout=0.5, gpu=0, lr=0.01, n_epochs=200, n_hidden=128, n_layers=1, self_loop=True, weight_decay=0.0005)\n",
      "----Data statistics------'\n",
      "      #Edges 31785\n",
      "      #Classes 3\n",
      "      #Train samples 100\n",
      "      #Val samples 300\n",
      "      #Test samples 600\n",
      "add self_loop\n",
      "GCN(\n",
      "  (layers): ModuleList(\n",
      "    (0): GraphConv(in=500, out=128, normalization=True, activation=<function relu at 0x7ff0c5a1e290>)\n",
      "    (1): GraphConv(in=128, out=3, normalization=True, activation=None)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Epoch 00000 | Time(s) nan | Loss 0.8976 | Accuracy 0.0000 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 0.8410 | Accuracy 0.0000 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 0.8002 | Accuracy 0.0000 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.0035 | Loss 0.7743 | Accuracy 0.0000 | ETputs(KTEPS) 18556.07\n",
      "Epoch 00004 | Time(s) 0.0035 | Loss 0.7393 | Accuracy 0.0000 | ETputs(KTEPS) 18522.70\n",
      "Epoch 00005 | Time(s) 0.0035 | Loss 0.7154 | Accuracy 0.0000 | ETputs(KTEPS) 18443.20\n",
      "Epoch 00006 | Time(s) 0.0035 | Loss 0.6830 | Accuracy 0.0000 | ETputs(KTEPS) 18470.04\n",
      "Epoch 00007 | Time(s) 0.0035 | Loss 0.6575 | Accuracy 0.0000 | ETputs(KTEPS) 18479.67\n",
      "Epoch 00008 | Time(s) 0.0035 | Loss 0.6299 | Accuracy 0.0000 | ETputs(KTEPS) 18493.82\n",
      "Epoch 00009 | Time(s) 0.0035 | Loss 0.6096 | Accuracy 0.0000 | ETputs(KTEPS) 18510.58\n",
      "Epoch 00010 | Time(s) 0.0035 | Loss 0.5905 | Accuracy 0.0000 | ETputs(KTEPS) 18461.92\n",
      "Epoch 00011 | Time(s) 0.0044 | Loss 0.5812 | Accuracy 0.0000 | ETputs(KTEPS) 14824.92\n",
      "Epoch 00012 | Time(s) 0.0043 | Loss 0.5617 | Accuracy 0.0000 | ETputs(KTEPS) 15114.88\n",
      "Epoch 00013 | Time(s) 0.0043 | Loss 0.5487 | Accuracy 0.0000 | ETputs(KTEPS) 15299.79\n",
      "Epoch 00014 | Time(s) 0.0042 | Loss 0.5367 | Accuracy 0.0000 | ETputs(KTEPS) 15546.01\n",
      "Epoch 00015 | Time(s) 0.0041 | Loss 0.5298 | Accuracy 0.0000 | ETputs(KTEPS) 15767.85\n",
      "Epoch 00016 | Time(s) 0.0041 | Loss 0.5218 | Accuracy 0.0000 | ETputs(KTEPS) 15966.03\n",
      "Epoch 00017 | Time(s) 0.0040 | Loss 0.5131 | Accuracy 0.0000 | ETputs(KTEPS) 16143.51\n",
      "Epoch 00018 | Time(s) 0.0040 | Loss 0.5031 | Accuracy 0.0000 | ETputs(KTEPS) 16286.27\n",
      "Epoch 00019 | Time(s) 0.0040 | Loss 0.4977 | Accuracy 0.0000 | ETputs(KTEPS) 16402.91\n",
      "Epoch 00020 | Time(s) 0.0039 | Loss 0.4966 | Accuracy 0.0000 | ETputs(KTEPS) 16512.89\n",
      "Epoch 00021 | Time(s) 0.0039 | Loss 0.4853 | Accuracy 0.0000 | ETputs(KTEPS) 16612.12\n",
      "Epoch 00022 | Time(s) 0.0039 | Loss 0.4776 | Accuracy 0.0000 | ETputs(KTEPS) 16699.24\n",
      "Epoch 00023 | Time(s) 0.0039 | Loss 0.4753 | Accuracy 0.0000 | ETputs(KTEPS) 16792.17\n",
      "Epoch 00024 | Time(s) 0.0039 | Loss 0.4737 | Accuracy 0.0000 | ETputs(KTEPS) 16799.02\n",
      "Epoch 00025 | Time(s) 0.0039 | Loss 0.4886 | Accuracy 0.0000 | ETputs(KTEPS) 16875.36\n",
      "Epoch 00026 | Time(s) 0.0038 | Loss 0.4734 | Accuracy 0.0000 | ETputs(KTEPS) 16945.51\n",
      "Epoch 00027 | Time(s) 0.0038 | Loss 0.4730 | Accuracy 0.0000 | ETputs(KTEPS) 17005.86\n",
      "Epoch 00028 | Time(s) 0.0038 | Loss 0.4679 | Accuracy 0.0000 | ETputs(KTEPS) 17069.14\n",
      "Epoch 00029 | Time(s) 0.0038 | Loss 0.4676 | Accuracy 0.0000 | ETputs(KTEPS) 17127.19\n",
      "Epoch 00030 | Time(s) 0.0038 | Loss 0.4627 | Accuracy 0.0000 | ETputs(KTEPS) 17166.55\n",
      "Epoch 00031 | Time(s) 0.0038 | Loss 0.4697 | Accuracy 0.0000 | ETputs(KTEPS) 17223.17\n",
      "Epoch 00032 | Time(s) 0.0038 | Loss 0.4572 | Accuracy 0.0000 | ETputs(KTEPS) 17273.57\n",
      "Epoch 00033 | Time(s) 0.0038 | Loss 0.4587 | Accuracy 0.0000 | ETputs(KTEPS) 17320.89\n",
      "Epoch 00034 | Time(s) 0.0037 | Loss 0.4603 | Accuracy 0.0000 | ETputs(KTEPS) 17365.21\n",
      "Epoch 00035 | Time(s) 0.0037 | Loss 0.4534 | Accuracy 0.0000 | ETputs(KTEPS) 17405.61\n",
      "Epoch 00036 | Time(s) 0.0037 | Loss 0.4565 | Accuracy 0.0000 | ETputs(KTEPS) 17438.94\n",
      "Epoch 00037 | Time(s) 0.0037 | Loss 0.4513 | Accuracy 0.0000 | ETputs(KTEPS) 17476.82\n",
      "Epoch 00038 | Time(s) 0.0037 | Loss 0.4552 | Accuracy 0.0000 | ETputs(KTEPS) 17493.07\n",
      "Epoch 00039 | Time(s) 0.0037 | Loss 0.4566 | Accuracy 0.0000 | ETputs(KTEPS) 17530.60\n",
      "Epoch 00040 | Time(s) 0.0037 | Loss 0.4479 | Accuracy 0.0000 | ETputs(KTEPS) 17566.57\n",
      "Epoch 00041 | Time(s) 0.0037 | Loss 0.4602 | Accuracy 0.0000 | ETputs(KTEPS) 17592.11\n",
      "Epoch 00042 | Time(s) 0.0037 | Loss 0.4520 | Accuracy 0.0000 | ETputs(KTEPS) 17604.47\n",
      "Epoch 00043 | Time(s) 0.0037 | Loss 0.4505 | Accuracy 0.0000 | ETputs(KTEPS) 17588.09\n",
      "Epoch 00044 | Time(s) 0.0037 | Loss 0.4507 | Accuracy 0.0000 | ETputs(KTEPS) 17603.74\n",
      "Epoch 00045 | Time(s) 0.0037 | Loss 0.4472 | Accuracy 0.0000 | ETputs(KTEPS) 17618.87\n",
      "Epoch 00046 | Time(s) 0.0037 | Loss 0.4527 | Accuracy 0.0000 | ETputs(KTEPS) 17611.59\n",
      "Epoch 00047 | Time(s) 0.0037 | Loss 0.4589 | Accuracy 0.0000 | ETputs(KTEPS) 17593.72\n",
      "Epoch 00048 | Time(s) 0.0037 | Loss 0.4625 | Accuracy 0.0000 | ETputs(KTEPS) 17610.69\n",
      "Epoch 00049 | Time(s) 0.0037 | Loss 0.4603 | Accuracy 0.0000 | ETputs(KTEPS) 17597.55\n",
      "Epoch 00050 | Time(s) 0.0037 | Loss 0.4579 | Accuracy 0.0000 | ETputs(KTEPS) 17568.10\n",
      "Epoch 00051 | Time(s) 0.0037 | Loss 0.4738 | Accuracy 0.0000 | ETputs(KTEPS) 17574.33\n",
      "Epoch 00052 | Time(s) 0.0037 | Loss 0.4479 | Accuracy 0.0000 | ETputs(KTEPS) 17594.63\n",
      "Epoch 00053 | Time(s) 0.0037 | Loss 0.4614 | Accuracy 0.0000 | ETputs(KTEPS) 17619.94\n",
      "Epoch 00054 | Time(s) 0.0037 | Loss 0.4513 | Accuracy 0.0000 | ETputs(KTEPS) 17645.50\n",
      "Epoch 00055 | Time(s) 0.0037 | Loss 0.4528 | Accuracy 0.0000 | ETputs(KTEPS) 17667.89\n",
      "Epoch 00056 | Time(s) 0.0037 | Loss 0.4557 | Accuracy 0.0000 | ETputs(KTEPS) 17683.23\n",
      "Epoch 00057 | Time(s) 0.0037 | Loss 0.4559 | Accuracy 0.0000 | ETputs(KTEPS) 17692.15\n",
      "Epoch 00058 | Time(s) 0.0037 | Loss 0.4472 | Accuracy 0.0000 | ETputs(KTEPS) 17715.41\n",
      "Epoch 00059 | Time(s) 0.0037 | Loss 0.4651 | Accuracy 0.0000 | ETputs(KTEPS) 17734.25\n",
      "Epoch 00060 | Time(s) 0.0037 | Loss 0.4445 | Accuracy 0.0000 | ETputs(KTEPS) 17753.89\n",
      "Epoch 00061 | Time(s) 0.0037 | Loss 0.4539 | Accuracy 0.0000 | ETputs(KTEPS) 17767.79\n",
      "Epoch 00062 | Time(s) 0.0037 | Loss 0.4600 | Accuracy 0.0000 | ETputs(KTEPS) 17781.04\n",
      "Epoch 00063 | Time(s) 0.0037 | Loss 0.4436 | Accuracy 0.0000 | ETputs(KTEPS) 17798.34\n",
      "Epoch 00064 | Time(s) 0.0037 | Loss 0.4602 | Accuracy 0.0000 | ETputs(KTEPS) 17815.57\n",
      "Epoch 00065 | Time(s) 0.0036 | Loss 0.4484 | Accuracy 0.0000 | ETputs(KTEPS) 17833.83\n",
      "Epoch 00066 | Time(s) 0.0036 | Loss 0.4468 | Accuracy 0.0000 | ETputs(KTEPS) 17852.74\n",
      "Epoch 00067 | Time(s) 0.0036 | Loss 0.4641 | Accuracy 0.0000 | ETputs(KTEPS) 17868.87\n",
      "Epoch 00068 | Time(s) 0.0036 | Loss 0.4487 | Accuracy 0.0000 | ETputs(KTEPS) 17867.92\n",
      "Epoch 00069 | Time(s) 0.0036 | Loss 0.4442 | Accuracy 0.0000 | ETputs(KTEPS) 17875.58\n",
      "Epoch 00070 | Time(s) 0.0036 | Loss 0.4617 | Accuracy 0.0000 | ETputs(KTEPS) 17883.17\n",
      "Epoch 00071 | Time(s) 0.0036 | Loss 0.4369 | Accuracy 0.0000 | ETputs(KTEPS) 17897.64\n",
      "Epoch 00072 | Time(s) 0.0036 | Loss 0.4577 | Accuracy 0.0000 | ETputs(KTEPS) 17913.37\n",
      "Epoch 00073 | Time(s) 0.0036 | Loss 0.4434 | Accuracy 0.0000 | ETputs(KTEPS) 17927.92\n",
      "Epoch 00074 | Time(s) 0.0036 | Loss 0.4348 | Accuracy 0.0000 | ETputs(KTEPS) 17943.18\n",
      "Epoch 00075 | Time(s) 0.0036 | Loss 0.4480 | Accuracy 0.0000 | ETputs(KTEPS) 17958.89\n",
      "Epoch 00076 | Time(s) 0.0036 | Loss 0.4417 | Accuracy 0.0000 | ETputs(KTEPS) 17971.39\n",
      "Epoch 00077 | Time(s) 0.0036 | Loss 0.4387 | Accuracy 0.0000 | ETputs(KTEPS) 17983.81\n",
      "Epoch 00078 | Time(s) 0.0036 | Loss 0.4468 | Accuracy 0.0000 | ETputs(KTEPS) 17997.07\n",
      "Epoch 00079 | Time(s) 0.0036 | Loss 0.4382 | Accuracy 0.0000 | ETputs(KTEPS) 18009.98\n",
      "Epoch 00080 | Time(s) 0.0036 | Loss 0.4388 | Accuracy 0.0000 | ETputs(KTEPS) 18026.07\n",
      "Epoch 00081 | Time(s) 0.0036 | Loss 0.4443 | Accuracy 0.0000 | ETputs(KTEPS) 18037.80\n",
      "Epoch 00082 | Time(s) 0.0036 | Loss 0.4395 | Accuracy 0.0000 | ETputs(KTEPS) 18054.04\n",
      "Epoch 00083 | Time(s) 0.0036 | Loss 0.4368 | Accuracy 0.0000 | ETputs(KTEPS) 18056.36\n",
      "Epoch 00084 | Time(s) 0.0036 | Loss 0.4398 | Accuracy 0.0000 | ETputs(KTEPS) 18069.97\n",
      "Epoch 00085 | Time(s) 0.0036 | Loss 0.4374 | Accuracy 0.0000 | ETputs(KTEPS) 18083.08\n",
      "Epoch 00086 | Time(s) 0.0036 | Loss 0.4400 | Accuracy 0.0000 | ETputs(KTEPS) 18075.63\n",
      "Epoch 00087 | Time(s) 0.0036 | Loss 0.4364 | Accuracy 0.0000 | ETputs(KTEPS) 18087.88\n",
      "Epoch 00088 | Time(s) 0.0036 | Loss 0.4399 | Accuracy 0.0000 | ETputs(KTEPS) 18100.68\n",
      "Epoch 00089 | Time(s) 0.0036 | Loss 0.4440 | Accuracy 0.0000 | ETputs(KTEPS) 18111.68\n",
      "Epoch 00090 | Time(s) 0.0036 | Loss 0.4323 | Accuracy 0.0000 | ETputs(KTEPS) 18121.32\n",
      "Epoch 00091 | Time(s) 0.0036 | Loss 0.4408 | Accuracy 0.0000 | ETputs(KTEPS) 18133.98\n",
      "Epoch 00092 | Time(s) 0.0036 | Loss 0.4354 | Accuracy 0.0000 | ETputs(KTEPS) 18118.88\n",
      "Epoch 00093 | Time(s) 0.0036 | Loss 0.4336 | Accuracy 0.0000 | ETputs(KTEPS) 18129.14\n",
      "Epoch 00094 | Time(s) 0.0036 | Loss 0.4336 | Accuracy 0.0000 | ETputs(KTEPS) 18136.85\n",
      "Epoch 00095 | Time(s) 0.0036 | Loss 0.4291 | Accuracy 0.0000 | ETputs(KTEPS) 18147.30\n",
      "Epoch 00096 | Time(s) 0.0036 | Loss 0.4315 | Accuracy 0.0000 | ETputs(KTEPS) 18157.64\n",
      "Epoch 00097 | Time(s) 0.0036 | Loss 0.4347 | Accuracy 0.0000 | ETputs(KTEPS) 18164.98\n",
      "Epoch 00098 | Time(s) 0.0036 | Loss 0.4332 | Accuracy 0.0000 | ETputs(KTEPS) 18174.91\n",
      "Epoch 00099 | Time(s) 0.0036 | Loss 0.4342 | Accuracy 0.0000 | ETputs(KTEPS) 18183.46\n",
      "Epoch 00100 | Time(s) 0.0036 | Loss 0.4412 | Accuracy 0.0000 | ETputs(KTEPS) 18193.36\n",
      "Epoch 00101 | Time(s) 0.0036 | Loss 0.4323 | Accuracy 0.0000 | ETputs(KTEPS) 18201.92\n",
      "Epoch 00102 | Time(s) 0.0036 | Loss 0.4453 | Accuracy 0.0000 | ETputs(KTEPS) 18208.47\n",
      "Epoch 00103 | Time(s) 0.0036 | Loss 0.4347 | Accuracy 0.0000 | ETputs(KTEPS) 18218.34\n",
      "Epoch 00104 | Time(s) 0.0036 | Loss 0.4332 | Accuracy 0.0000 | ETputs(KTEPS) 18228.39\n",
      "Epoch 00105 | Time(s) 0.0036 | Loss 0.4455 | Accuracy 0.0000 | ETputs(KTEPS) 18234.41\n",
      "Epoch 00106 | Time(s) 0.0036 | Loss 0.4334 | Accuracy 0.0000 | ETputs(KTEPS) 18225.85\n",
      "Epoch 00107 | Time(s) 0.0036 | Loss 0.4370 | Accuracy 0.0000 | ETputs(KTEPS) 18232.30\n",
      "Epoch 00108 | Time(s) 0.0036 | Loss 0.4371 | Accuracy 0.0000 | ETputs(KTEPS) 18239.00\n",
      "Epoch 00109 | Time(s) 0.0036 | Loss 0.4268 | Accuracy 0.0000 | ETputs(KTEPS) 18245.91\n",
      "Epoch 00110 | Time(s) 0.0036 | Loss 0.4443 | Accuracy 0.0000 | ETputs(KTEPS) 18259.43\n",
      "Epoch 00111 | Time(s) 0.0036 | Loss 0.4319 | Accuracy 0.0000 | ETputs(KTEPS) 18267.93\n",
      "Epoch 00112 | Time(s) 0.0036 | Loss 0.4356 | Accuracy 0.0000 | ETputs(KTEPS) 18277.44\n",
      "Epoch 00113 | Time(s) 0.0036 | Loss 0.4347 | Accuracy 0.0000 | ETputs(KTEPS) 18285.38\n",
      "Epoch 00114 | Time(s) 0.0036 | Loss 0.4298 | Accuracy 0.0000 | ETputs(KTEPS) 18292.36\n",
      "Epoch 00115 | Time(s) 0.0036 | Loss 0.4302 | Accuracy 0.0000 | ETputs(KTEPS) 18298.16\n",
      "Epoch 00116 | Time(s) 0.0036 | Loss 0.4313 | Accuracy 0.0000 | ETputs(KTEPS) 18305.30\n",
      "Epoch 00117 | Time(s) 0.0036 | Loss 0.4280 | Accuracy 0.0000 | ETputs(KTEPS) 18312.26\n",
      "Epoch 00118 | Time(s) 0.0036 | Loss 0.4264 | Accuracy 0.0000 | ETputs(KTEPS) 18317.19\n",
      "Epoch 00119 | Time(s) 0.0036 | Loss 0.4309 | Accuracy 0.0000 | ETputs(KTEPS) 18319.49\n",
      "Epoch 00120 | Time(s) 0.0036 | Loss 0.4317 | Accuracy 0.0000 | ETputs(KTEPS) 18325.96\n",
      "Epoch 00121 | Time(s) 0.0035 | Loss 0.4308 | Accuracy 0.0000 | ETputs(KTEPS) 18333.16\n",
      "Epoch 00122 | Time(s) 0.0035 | Loss 0.4320 | Accuracy 0.0000 | ETputs(KTEPS) 18339.97\n",
      "Epoch 00123 | Time(s) 0.0035 | Loss 0.4268 | Accuracy 0.0000 | ETputs(KTEPS) 18345.08\n",
      "Epoch 00124 | Time(s) 0.0035 | Loss 0.4332 | Accuracy 0.0000 | ETputs(KTEPS) 18349.90\n",
      "Epoch 00125 | Time(s) 0.0035 | Loss 0.4258 | Accuracy 0.0000 | ETputs(KTEPS) 18352.67\n",
      "Epoch 00126 | Time(s) 0.0035 | Loss 0.4330 | Accuracy 0.0000 | ETputs(KTEPS) 18355.79\n",
      "Epoch 00127 | Time(s) 0.0035 | Loss 0.4289 | Accuracy 0.0000 | ETputs(KTEPS) 18357.78\n",
      "Epoch 00128 | Time(s) 0.0035 | Loss 0.4381 | Accuracy 0.0000 | ETputs(KTEPS) 18361.47\n",
      "Epoch 00129 | Time(s) 0.0035 | Loss 0.4307 | Accuracy 0.0000 | ETputs(KTEPS) 18364.29\n",
      "Epoch 00130 | Time(s) 0.0035 | Loss 0.4390 | Accuracy 0.0000 | ETputs(KTEPS) 18367.55\n",
      "Epoch 00131 | Time(s) 0.0035 | Loss 0.4325 | Accuracy 0.0000 | ETputs(KTEPS) 18371.02\n",
      "Epoch 00132 | Time(s) 0.0035 | Loss 0.4301 | Accuracy 0.0000 | ETputs(KTEPS) 18376.34\n",
      "Epoch 00133 | Time(s) 0.0035 | Loss 0.4273 | Accuracy 0.0000 | ETputs(KTEPS) 18381.53\n",
      "Epoch 00134 | Time(s) 0.0035 | Loss 0.4335 | Accuracy 0.0000 | ETputs(KTEPS) 18382.93\n",
      "Epoch 00135 | Time(s) 0.0035 | Loss 0.4268 | Accuracy 0.0000 | ETputs(KTEPS) 18388.06\n",
      "Epoch 00136 | Time(s) 0.0035 | Loss 0.4318 | Accuracy 0.0000 | ETputs(KTEPS) 18394.04\n",
      "Epoch 00137 | Time(s) 0.0035 | Loss 0.4350 | Accuracy 0.0000 | ETputs(KTEPS) 18399.27\n",
      "Epoch 00138 | Time(s) 0.0035 | Loss 0.4333 | Accuracy 0.0000 | ETputs(KTEPS) 18402.42\n",
      "Epoch 00139 | Time(s) 0.0035 | Loss 0.4307 | Accuracy 0.0000 | ETputs(KTEPS) 18409.13\n",
      "Epoch 00140 | Time(s) 0.0035 | Loss 0.4303 | Accuracy 0.0000 | ETputs(KTEPS) 18415.22\n",
      "Epoch 00141 | Time(s) 0.0035 | Loss 0.4395 | Accuracy 0.0000 | ETputs(KTEPS) 18419.63\n",
      "Epoch 00142 | Time(s) 0.0035 | Loss 0.4260 | Accuracy 0.0000 | ETputs(KTEPS) 18423.15\n",
      "Epoch 00143 | Time(s) 0.0035 | Loss 0.4465 | Accuracy 0.0000 | ETputs(KTEPS) 18428.30\n",
      "Epoch 00144 | Time(s) 0.0035 | Loss 0.4330 | Accuracy 0.0000 | ETputs(KTEPS) 18433.77\n",
      "Epoch 00145 | Time(s) 0.0035 | Loss 0.4338 | Accuracy 0.0000 | ETputs(KTEPS) 18438.13\n",
      "Epoch 00146 | Time(s) 0.0035 | Loss 0.4316 | Accuracy 0.0000 | ETputs(KTEPS) 18442.14\n",
      "Epoch 00147 | Time(s) 0.0035 | Loss 0.4312 | Accuracy 0.0000 | ETputs(KTEPS) 18447.95\n",
      "Epoch 00148 | Time(s) 0.0035 | Loss 0.4366 | Accuracy 0.0000 | ETputs(KTEPS) 18451.57\n",
      "Epoch 00149 | Time(s) 0.0035 | Loss 0.4252 | Accuracy 0.0000 | ETputs(KTEPS) 18455.56\n",
      "Epoch 00150 | Time(s) 0.0035 | Loss 0.4339 | Accuracy 0.0000 | ETputs(KTEPS) 18459.77\n",
      "Epoch 00151 | Time(s) 0.0035 | Loss 0.4289 | Accuracy 0.0000 | ETputs(KTEPS) 18456.41\n",
      "Epoch 00152 | Time(s) 0.0035 | Loss 0.4247 | Accuracy 0.0000 | ETputs(KTEPS) 18460.59\n",
      "Epoch 00153 | Time(s) 0.0035 | Loss 0.4321 | Accuracy 0.0000 | ETputs(KTEPS) 18466.36\n",
      "Epoch 00154 | Time(s) 0.0035 | Loss 0.4277 | Accuracy 0.0000 | ETputs(KTEPS) 18473.79\n",
      "Epoch 00155 | Time(s) 0.0035 | Loss 0.4375 | Accuracy 0.0000 | ETputs(KTEPS) 18475.28\n",
      "Epoch 00156 | Time(s) 0.0035 | Loss 0.4289 | Accuracy 0.0000 | ETputs(KTEPS) 18480.47\n",
      "Epoch 00157 | Time(s) 0.0035 | Loss 0.4294 | Accuracy 0.0000 | ETputs(KTEPS) 18485.34\n",
      "Epoch 00158 | Time(s) 0.0035 | Loss 0.4275 | Accuracy 0.0000 | ETputs(KTEPS) 18490.65\n",
      "Epoch 00159 | Time(s) 0.0035 | Loss 0.4283 | Accuracy 0.0000 | ETputs(KTEPS) 18494.92\n",
      "Epoch 00160 | Time(s) 0.0035 | Loss 0.4332 | Accuracy 0.0000 | ETputs(KTEPS) 18499.65\n",
      "Epoch 00161 | Time(s) 0.0035 | Loss 0.4269 | Accuracy 0.0000 | ETputs(KTEPS) 18504.54\n",
      "Epoch 00162 | Time(s) 0.0035 | Loss 0.4337 | Accuracy 0.0000 | ETputs(KTEPS) 18504.20\n",
      "Epoch 00163 | Time(s) 0.0035 | Loss 0.4292 | Accuracy 0.0000 | ETputs(KTEPS) 18506.60\n",
      "Epoch 00164 | Time(s) 0.0035 | Loss 0.4411 | Accuracy 0.0000 | ETputs(KTEPS) 18512.33\n",
      "Epoch 00165 | Time(s) 0.0035 | Loss 0.4282 | Accuracy 0.0000 | ETputs(KTEPS) 18517.76\n",
      "Epoch 00166 | Time(s) 0.0035 | Loss 0.4310 | Accuracy 0.0000 | ETputs(KTEPS) 18522.44\n",
      "Epoch 00167 | Time(s) 0.0035 | Loss 0.4328 | Accuracy 0.0000 | ETputs(KTEPS) 18527.57\n",
      "Epoch 00168 | Time(s) 0.0035 | Loss 0.4306 | Accuracy 0.0000 | ETputs(KTEPS) 18532.77\n",
      "Epoch 00169 | Time(s) 0.0035 | Loss 0.4247 | Accuracy 0.0000 | ETputs(KTEPS) 18538.33\n",
      "Epoch 00170 | Time(s) 0.0035 | Loss 0.4245 | Accuracy 0.0000 | ETputs(KTEPS) 18542.77\n",
      "Epoch 00171 | Time(s) 0.0035 | Loss 0.4249 | Accuracy 0.0000 | ETputs(KTEPS) 18546.98\n",
      "Epoch 00172 | Time(s) 0.0035 | Loss 0.4236 | Accuracy 0.0000 | ETputs(KTEPS) 18552.01\n",
      "Epoch 00173 | Time(s) 0.0035 | Loss 0.4241 | Accuracy 0.0000 | ETputs(KTEPS) 18556.44\n",
      "Epoch 00174 | Time(s) 0.0035 | Loss 0.4248 | Accuracy 0.0000 | ETputs(KTEPS) 18560.71\n",
      "Epoch 00175 | Time(s) 0.0035 | Loss 0.4280 | Accuracy 0.0000 | ETputs(KTEPS) 18564.11\n",
      "Epoch 00176 | Time(s) 0.0035 | Loss 0.4254 | Accuracy 0.0000 | ETputs(KTEPS) 18569.06\n",
      "Epoch 00177 | Time(s) 0.0035 | Loss 0.4271 | Accuracy 0.0000 | ETputs(KTEPS) 18573.35\n",
      "Epoch 00178 | Time(s) 0.0035 | Loss 0.4300 | Accuracy 0.0000 | ETputs(KTEPS) 18577.27\n",
      "Epoch 00179 | Time(s) 0.0035 | Loss 0.4236 | Accuracy 0.0000 | ETputs(KTEPS) 18580.02\n",
      "Epoch 00180 | Time(s) 0.0035 | Loss 0.4307 | Accuracy 0.0000 | ETputs(KTEPS) 18579.57\n",
      "Epoch 00181 | Time(s) 0.0035 | Loss 0.4318 | Accuracy 0.0000 | ETputs(KTEPS) 18568.48\n",
      "Epoch 00182 | Time(s) 0.0035 | Loss 0.4301 | Accuracy 0.0000 | ETputs(KTEPS) 18570.04\n",
      "Epoch 00183 | Time(s) 0.0035 | Loss 0.4236 | Accuracy 0.0000 | ETputs(KTEPS) 18564.97\n",
      "Epoch 00184 | Time(s) 0.0035 | Loss 0.4226 | Accuracy 0.0000 | ETputs(KTEPS) 18567.90\n",
      "Epoch 00185 | Time(s) 0.0035 | Loss 0.4238 | Accuracy 0.0000 | ETputs(KTEPS) 18571.68\n",
      "Epoch 00186 | Time(s) 0.0035 | Loss 0.4239 | Accuracy 0.0000 | ETputs(KTEPS) 18574.63\n",
      "Epoch 00187 | Time(s) 0.0035 | Loss 0.4244 | Accuracy 0.0000 | ETputs(KTEPS) 18576.88\n",
      "Epoch 00188 | Time(s) 0.0035 | Loss 0.4277 | Accuracy 0.0000 | ETputs(KTEPS) 18568.50\n",
      "Epoch 00189 | Time(s) 0.0035 | Loss 0.4260 | Accuracy 0.0000 | ETputs(KTEPS) 18558.92\n",
      "Epoch 00190 | Time(s) 0.0035 | Loss 0.4278 | Accuracy 0.0000 | ETputs(KTEPS) 18559.58\n",
      "Epoch 00191 | Time(s) 0.0035 | Loss 0.4218 | Accuracy 0.0000 | ETputs(KTEPS) 18555.11\n",
      "Epoch 00192 | Time(s) 0.0035 | Loss 0.4242 | Accuracy 0.0000 | ETputs(KTEPS) 18561.33\n",
      "Epoch 00193 | Time(s) 0.0035 | Loss 0.4260 | Accuracy 0.0000 | ETputs(KTEPS) 18558.43\n",
      "Epoch 00194 | Time(s) 0.0035 | Loss 0.4244 | Accuracy 0.0000 | ETputs(KTEPS) 18555.66\n",
      "Epoch 00195 | Time(s) 0.0035 | Loss 0.4245 | Accuracy 0.0000 | ETputs(KTEPS) 18555.54\n",
      "Epoch 00196 | Time(s) 0.0035 | Loss 0.4191 | Accuracy 0.0000 | ETputs(KTEPS) 18556.35\n",
      "Epoch 00197 | Time(s) 0.0035 | Loss 0.4248 | Accuracy 0.0000 | ETputs(KTEPS) 18557.79\n",
      "Epoch 00198 | Time(s) 0.0035 | Loss 0.4210 | Accuracy 0.0000 | ETputs(KTEPS) 18559.45\n",
      "Epoch 00199 | Time(s) 0.0035 | Loss 0.4274 | Accuracy 0.0000 | ETputs(KTEPS) 18561.26\n",
      "\n",
      "torch.Size([7903, 128])\n",
      "Test accuracy 0.00%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='GCN')\n",
    "#     register_data_args(parser)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.5,\n",
    "            help=\"dropout probability\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "            help=\"gpu\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-2,\n",
    "            help=\"learning rate\")\n",
    "    parser.add_argument(\"--n-epochs\", type=int, default=200,\n",
    "            help=\"number of training epochs\")\n",
    "    parser.add_argument(\"--n-hidden\", type=int, default=128,\n",
    "            help=\"number of hidden gcn units\")\n",
    "    parser.add_argument(\"--n-layers\", type=int, default=1,\n",
    "            help=\"number of hidden gcn layers\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=5e-4,\n",
    "            help=\"Weight for L2 loss\")\n",
    "    parser.add_argument(\"--self-loop\", action='store_true',\n",
    "            help=\"graph self-loop (default=False)\")\n",
    "#     parser.add_argument(\"--dataset\", default='cora')\n",
    "    parser.set_defaults(self_loop=True)\n",
    "    parser.add_argument(\"--dataset\", default='cora')\n",
    "#     args = parser.parse_args()\n",
    "#     args = parser.parse_args()[1:]\n",
    "    args = parser.parse_known_args()[0]\n",
    "    print(args)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments\n",
    "dataset | level | shape | acc\n",
    "--- | --- | --- | ---\n",
    "cora | 0 | 81.1\n",
    "citeseer | 1402, 128 | 65.20\n",
    "pubmed | 7903, 128 | 79.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1402, 128)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emb_cora_l1 = np.load('../graphzoom/embed_results/cora/cora_level_2.npy')\n",
    "emb_cora_l1 = np.load('citeseer_emb_level_1.npy')\n",
    "emb_cora_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8133336 , 1.074242  , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.9198053 , ..., 1.4515474 , 0.25990957,\n",
       "        0.        ],\n",
       "       [0.82546675, 0.        , 0.        , ..., 0.17736198, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 1.5836211 , ..., 1.0919161 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 2.675375  , ..., 1.2003655 , 0.36214033,\n",
       "        0.        ],\n",
       "       [0.6243051 , 0.        , 0.        , ..., 0.        , 1.064039  ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emb_cora_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/yushi/repo/GraphZoom/dgl_gcn\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citeseer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
