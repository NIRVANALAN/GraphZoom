{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, citegrh\n",
    "\n",
    "# from gcn import GCN\n",
    "#from gcn_mp import GCN\n",
    "#from gcn_spmv import GCN\n",
    "# from graphsage_utils import *\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "def load_data_dgl(dataset='cora'):\n",
    "    if dataset == 'cora':\n",
    "        return citegrh.load_cora()\n",
    "    elif dataset == 'citeseer':\n",
    "        return citegrh.load_citeseer()\n",
    "    elif dataset == 'pubmed':\n",
    "        return citegrh.load_pubmed()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GCN using DGL nn package\n",
    "\n",
    "References:\n",
    "- Semi-Supervised Classification with Graph Convolutional Networks\n",
    "- Paper: https://arxiv.org/abs/1609.02907\n",
    "- Code: https://github.com/tkipf/gcn\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout, log_softmax=False):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.log_softmax = log_softmax\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            emb = h\n",
    "            h = layer(self.g, h)\n",
    "        if self.log_softmax:\n",
    "            return nn.functional.log_softmax(h, 1), emb\n",
    "        return h, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix, normalize=True, load_walks=False):\n",
    "    G_data = json.load(open(prefix + \"-G.json\"))\n",
    "    G = json_graph.node_link_graph(G_data)\n",
    "    if isinstance(G.nodes()[0], int):\n",
    "        def conversion(n): return int(n)\n",
    "    else:\n",
    "        def conversion(n): return n\n",
    "\n",
    "    if os.path.exists(prefix + \"-feats.npy\"):\n",
    "        feats = np.load(prefix + \"-feats.npy\")\n",
    "    else:\n",
    "        print(\"No features present.. Only identity features will be used.\")\n",
    "        feats = None\n",
    "    class_map = json.load(open(prefix + \"-class_map.json\"))\n",
    "    if isinstance(list(class_map.values())[0], list):\n",
    "        def lab_conversion(n): return n\n",
    "    else:\n",
    "        def lab_conversion(n): return int(n)\n",
    "\n",
    "    class_map = {conversion(k): lab_conversion(v)\n",
    "                 for k, v in class_map.items()}\n",
    "\n",
    "    # Remove all nodes that do not have val/test annotations\n",
    "    # (necessary because of networkx weirdness with the Reddit data)\n",
    "    broken_count = 0\n",
    "    for node in G.nodes():\n",
    "        if not 'val' in G.node[node] or not 'test' in G.node[node]:\n",
    "            G.remove_node(node)\n",
    "            broken_count += 1\n",
    "    print(\"Removed {:d} nodes that lacked proper annotations due to networkx versioning issues\".format(\n",
    "        broken_count))\n",
    "\n",
    "    # Make sure the graph has edge train_removed annotations\n",
    "    # (some datasets might already have this..)\n",
    "    print(\"Loaded data.. now preprocessing..\")\n",
    "    for edge in G.edges():\n",
    "        if (G.node[edge[0]]['val'] or G.node[edge[1]]['val'] or\n",
    "                G.node[edge[0]]['test'] or G.node[edge[1]]['test']):\n",
    "            G[edge[0]][edge[1]]['train_removed'] = True\n",
    "        else:\n",
    "            G[edge[0]][edge[1]]['train_removed'] = False\n",
    "\n",
    "    if normalize and not feats is None:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        train_ids = np.array([n for n in G.nodes(\n",
    "        ) if not G.node[n]['val'] and not G.node[n]['test']])\n",
    "        train_feats = feats[train_ids]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_feats)\n",
    "        feats = scaler.transform(feats)\n",
    "\n",
    "    return G, feats, class_map\n",
    "\n",
    "def _sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "1000\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "# train_prefix = '../graphzoom/dataset/cora/cora'\n",
    "# G, features, class_map = load_data(train_prefix)\n",
    "dataset = 'citeseer'\n",
    "dataset_dir = f'../graphzoom/dataset/{dataset}'\n",
    "G      = json_graph.node_link_graph(json.load(open(dataset_dir + \"/{}-G.json\".format(dataset))))\n",
    "labels = json.load(open(dataset_dir + \"/{}-class_map.json\".format(dataset)))\n",
    "feats = np.load(dataset_dir + f\"/{dataset}-feats.npy\")\n",
    "\n",
    "train_ids    = [n for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']]\n",
    "test_ids     = [n for n in G.nodes() if G.node[n]['test']]\n",
    "val_ids     = test_ids[1000:1500]\n",
    "test_ids     = test_ids[:1000]\n",
    "# train_labels = [labels[str(i)] for i in train_ids]\n",
    "# test_labels  = [labels[str(i)] for i in test_ids]\n",
    "labels = torch.LongTensor(list(labels.values()))\n",
    "train_mask = _sample_mask(train_ids, labels.shape[0])\n",
    "test_mask =  _sample_mask(test_ids, labels.shape[0])\n",
    "val_mask =  _sample_mask(val_ids, labels.shape[0])\n",
    "# val_mask = _sample_mask(range(200, 500), labels.shape[0])\n",
    "onehot_labels = F.one_hot(labels)\n",
    "print(len(train_labels))\n",
    "print(len(test_ids))\n",
    "print(len(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EasyDict({\n",
    "    'graph': G,\n",
    "    'labels': labels,\n",
    "    'onehot_labels': onehot_labels,\n",
    "    'features': feats,\n",
    "    'train_mask':train_mask,\n",
    "    'val_mask': val_mask,\n",
    "    'test_mask': test_mask,\n",
    "    'num_classes': onehot_labels.shape[1],\n",
    "    'coarse': False\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def construct_proj_laplacian(laplacian, levels, proj_dir):\n",
    "    coarse_laplacian = []\n",
    "    projections = []\n",
    "    for i in range(levels):\n",
    "        projection_name = \"{}/Projection_{}.mtx\".format(proj_dir, i+1)\n",
    "        projection = mtx2matrix(projection_name)\n",
    "        projections.append(projection)\n",
    "        coarse_laplacian.append(laplacian)\n",
    "        if i != (levels-1):\n",
    "            laplacian = projection @ laplacian @ (projection.transpose())\n",
    "    return projections, coarse_laplacian\n",
    "\n",
    "def mtx2matrix(proj_name):\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    with open(proj_name) as ff:\n",
    "        for i, line in enumerate(ff):\n",
    "            info = line.split()\n",
    "            if i == 0:\n",
    "                NumReducedNodes = int(info[0])\n",
    "                NumOriginNodes = int(info[1])\n",
    "            else:\n",
    "                row.append(int(info[0])-1)\n",
    "                col.append(int(info[1])-1)\n",
    "                data.append(1)\n",
    "    matrix = csr_matrix((data, (row, col)), shape=(\n",
    "        NumReducedNodes, NumOriginNodes))\n",
    "    return matrix\n",
    "levels = 2\n",
    "reduce_results = f\"../graphzoom/reduction_results/{dataset}\"\n",
    "original_adj = nx.adj_matrix(G)\n",
    "projections, coarse_adj = construct_proj_laplacian(\n",
    "    original_adj, levels, reduce_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "# softmax(labels)\n",
    "# level = 1\n",
    "coarse_feats = projections[0] @ data.features\n",
    "coarse_labels = projections[0] @ data.onehot_labels \n",
    "coarse_graph = nx.Graph(coarse_adj[1])\n",
    "rows_sum = coarse_labels.sum(axis=1)[:, np.newaxis]\n",
    "norm_coarse_labels = coarse_labels / rows_sum\n",
    "# list(map(np.shape, [coarse_embed, coarse_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_train_mask = _sample_mask(range(100), norm_coarse_labels.shape[0])\n",
    "coarse_test_mask = _sample_mask(range(100,700), norm_coarse_labels.shape[0])\n",
    "coarse_val_mask = _sample_mask(range(700,1000), norm_coarse_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_data = EasyDict({\n",
    "    'graph': coarse_graph,\n",
    "    'labels': coarse_labels,\n",
    "#     'onehot_labels': onehot_labels,\n",
    "    'features': coarse_feats,\n",
    "    'train_mask':coarse_train_mask,\n",
    "    'val_mask': coarse_val_mask,\n",
    "    'test_mask': coarse_test_mask,\n",
    "    'num_classes': norm_coarse_labels.shape[1],\n",
    "    'coarse' : True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1402, 3703)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = coarse_data\n",
    "data.val_mask.shape\n",
    "data.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import log_softmax\n",
    "import pdb\n",
    "def main(args):\n",
    "    # load and preprocess dataset\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    if data.coarse:\n",
    "        labels = torch.FloatTensor(data.labels)\n",
    "        loss_fcn = torch.nn.KLDivLoss()\n",
    "    else:\n",
    "        labels = torch.LongTensor(data.labels)\n",
    "        loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "#     g, features, class_map = load_data(train_prefix)\n",
    "#     labels = torch.LongTensor(list(class_map.values()))\n",
    "    if hasattr(torch, 'BoolTensor'):\n",
    "        train_mask = torch.BoolTensor(data.train_mask)\n",
    "        val_mask = torch.BoolTensor(data.val_mask)\n",
    "        test_mask = torch.BoolTensor(data.test_mask)\n",
    "    in_feats = data.features.shape[1]\n",
    "    n_classes = data.num_classes\n",
    "    n_edges = data.graph.number_of_edges()\n",
    "    print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "              train_mask.int().sum().item(),\n",
    "              val_mask.int().sum().item(),\n",
    "              test_mask.int().sum().item()))\n",
    "\n",
    "    if args.gpu < 0:\n",
    "        cuda = False\n",
    "    else:\n",
    "        cuda = True\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    # graph preprocess and calculate normalization factor\n",
    "    g = data.graph\n",
    "    # add self loop\n",
    "    if args.self_loop:\n",
    "        print('add self_loop')\n",
    "        g.remove_edges_from(nx.selfloop_edges(g))\n",
    "        g.add_edges_from(zip(g.nodes(), g.nodes()))\n",
    "    g = DGLGraph(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "    # normalization\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    if cuda:\n",
    "        norm = norm.cuda()\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    # create GCN model\n",
    "    model = GCN(g,\n",
    "                in_feats,\n",
    "                args.n_hidden,\n",
    "                n_classes,\n",
    "                args.n_layers,\n",
    "                F.relu,\n",
    "                args.dropout,log_softmax=data.coarse)\n",
    "    print(model)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(args.n_epochs):\n",
    "        model.train()\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        # forward\n",
    "        logits, h = model(features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "        acc = evaluate(model, features, labels, val_mask)\n",
    "#         acc=0\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\n",
    "              \"ETputs(KTEPS) {:.2f}\". format(epoch, np.mean(dur), loss.item(),\n",
    "                                             acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    print()\n",
    "    print(h.shape)\n",
    "    np.save(f'{dataset}_emb_level_1', h.detach().cpu().numpy())\n",
    "    acc = evaluate(model, features, labels, test_mask)\n",
    "    print(\"Test accuracy {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='cora', dropout=0.5, gpu=0, lr=0.01, n_epochs=200, n_hidden=128, n_layers=1, self_loop=True, weight_decay=0.0005)\n",
      "----Data statistics------'\n",
      "      #Edges 3371\n",
      "      #Classes 6\n",
      "      #Train samples 100\n",
      "      #Val samples 300\n",
      "      #Test samples 600\n",
      "add self_loop\n",
      "GCN(\n",
      "  (layers): ModuleList(\n",
      "    (0): GraphConv(in=3703, out=128, normalization=True, activation=<function relu at 0x7ff0c5a1e290>)\n",
      "    (1): GraphConv(in=128, out=6, normalization=True, activation=None)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-312-d95593a6e9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-311-d111c6f2d5ad>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dgl/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dgl/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dgl/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dgl/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1822\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='GCN')\n",
    "#     register_data_args(parser)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.5,\n",
    "            help=\"dropout probability\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "            help=\"gpu\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-2,\n",
    "            help=\"learning rate\")\n",
    "    parser.add_argument(\"--n-epochs\", type=int, default=200,\n",
    "            help=\"number of training epochs\")\n",
    "    parser.add_argument(\"--n-hidden\", type=int, default=128,\n",
    "            help=\"number of hidden gcn units\")\n",
    "    parser.add_argument(\"--n-layers\", type=int, default=1,\n",
    "            help=\"number of hidden gcn layers\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=5e-4,\n",
    "            help=\"Weight for L2 loss\")\n",
    "    parser.add_argument(\"--self-loop\", action='store_true',\n",
    "            help=\"graph self-loop (default=False)\")\n",
    "#     parser.add_argument(\"--dataset\", default='cora')\n",
    "    parser.set_defaults(self_loop=True)\n",
    "    parser.add_argument(\"--dataset\", default='cora')\n",
    "#     args = parser.parse_args()\n",
    "#     args = parser.parse_args()[1:]\n",
    "    args = parser.parse_known_args()[0]\n",
    "    print(args)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments\n",
    "dataset | level | shape | acc\n",
    "--- | --- | --- | ---\n",
    "cora | 0 | 81.1\n",
    "citeseer | 1402, 128 | 65.20\n",
    "pubmed | 7903, 128 | 79.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19717, 128)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emb_cora_l1 = np.load('../graphzoom/embed_results/cora/cora_level_2.npy')\n",
    "emb_cora_l1 = np.load('pubmed_emb_level_1.npy')\n",
    "emb_cora_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8133336 , 1.074242  , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.9198053 , ..., 1.4515474 , 0.25990957,\n",
       "        0.        ],\n",
       "       [0.82546675, 0.        , 0.        , ..., 0.17736198, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 1.5836211 , ..., 1.0919161 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 2.675375  , ..., 1.2003655 , 0.36214033,\n",
       "        0.        ],\n",
       "       [0.6243051 , 0.        , 0.        , ..., 0.        , 1.064039  ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emb_cora_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/yushi/repo/GraphZoom/dgl_gcn\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citeseer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
