{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import register_data_args, citegrh\n",
    "\n",
    "from gcn import GCN\n",
    "#from gcn_mp import GCN\n",
    "#from gcn_spmv import GCN\n",
    "# from graphsage_utils import *\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "def load_data_dgl(dataset='cora'):\n",
    "    if dataset == 'cora':\n",
    "        return citegrh.load_cora()\n",
    "    elif dataset == 'citeseer':\n",
    "        return citegrh.load_citeseer()\n",
    "    elif dataset == 'pubmed':\n",
    "        return citegrh.load_pubmed()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix, normalize=True, load_walks=False):\n",
    "    G_data = json.load(open(prefix + \"-G.json\"))\n",
    "    G = json_graph.node_link_graph(G_data)\n",
    "    if isinstance(G.nodes()[0], int):\n",
    "        def conversion(n): return int(n)\n",
    "    else:\n",
    "        def conversion(n): return n\n",
    "\n",
    "    if os.path.exists(prefix + \"-feats.npy\"):\n",
    "        feats = np.load(prefix + \"-feats.npy\")\n",
    "    else:\n",
    "        print(\"No features present.. Only identity features will be used.\")\n",
    "        feats = None\n",
    "    class_map = json.load(open(prefix + \"-class_map.json\"))\n",
    "    if isinstance(list(class_map.values())[0], list):\n",
    "        def lab_conversion(n): return n\n",
    "    else:\n",
    "        def lab_conversion(n): return int(n)\n",
    "\n",
    "    class_map = {conversion(k): lab_conversion(v)\n",
    "                 for k, v in class_map.items()}\n",
    "\n",
    "    # Remove all nodes that do not have val/test annotations\n",
    "    # (necessary because of networkx weirdness with the Reddit data)\n",
    "    broken_count = 0\n",
    "    for node in G.nodes():\n",
    "        if not 'val' in G.node[node] or not 'test' in G.node[node]:\n",
    "            G.remove_node(node)\n",
    "            broken_count += 1\n",
    "    print(\"Removed {:d} nodes that lacked proper annotations due to networkx versioning issues\".format(\n",
    "        broken_count))\n",
    "\n",
    "    # Make sure the graph has edge train_removed annotations\n",
    "    # (some datasets might already have this..)\n",
    "    print(\"Loaded data.. now preprocessing..\")\n",
    "    for edge in G.edges():\n",
    "        if (G.node[edge[0]]['val'] or G.node[edge[1]]['val'] or\n",
    "                G.node[edge[0]]['test'] or G.node[edge[1]]['test']):\n",
    "            G[edge[0]][edge[1]]['train_removed'] = True\n",
    "        else:\n",
    "            G[edge[0]][edge[1]]['train_removed'] = False\n",
    "\n",
    "    if normalize and not feats is None:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        train_ids = np.array([n for n in G.nodes(\n",
    "        ) if not G.node[n]['val'] and not G.node[n]['test']])\n",
    "        train_feats = feats[train_ids]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_feats)\n",
    "        feats = scaler.transform(feats)\n",
    "\n",
    "    return G, feats, class_map\n",
    "\n",
    "def _sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "1000\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "# train_prefix = '../graphzoom/dataset/cora/cora'\n",
    "# G, features, class_map = load_data(train_prefix)\n",
    "dataset = 'cora'\n",
    "dataset_dir = f'../graphzoom/dataset/{dataset}'\n",
    "G      = json_graph.node_link_graph(json.load(open(dataset_dir + \"/{}-G.json\".format(dataset))))\n",
    "labels = json.load(open(dataset_dir + \"/{}-class_map.json\".format(dataset)))\n",
    "feats = np.load(dataset_dir + f\"/{dataset}-feats.npy\")\n",
    "\n",
    "train_ids    = [n for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']]\n",
    "test_ids     = [n for n in G.nodes() if G.node[n]['test']]\n",
    "val_ids     = test_ids[1000:1500]\n",
    "test_ids     = test_ids[:1000]\n",
    "# train_labels = [labels[str(i)] for i in train_ids]\n",
    "# test_labels  = [labels[str(i)] for i in test_ids]\n",
    "labels = torch.LongTensor(list(class_map.values()))\n",
    "train_mask = _sample_mask(train_ids, labels.shape[0])\n",
    "test_mask =  _sample_mask(test_ids, labels.shape[0])\n",
    "val_mask =  _sample_mask(val_ids, labels.shape[0])\n",
    "# val_mask = _sample_mask(range(200, 500), labels.shape[0])\n",
    "onehot_labels = F.one_hot(labels)\n",
    "print(len(train_labels))\n",
    "print(len(test_ids))\n",
    "print(len(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
       "         11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
       "         22,   23,   24,   25,   26,   27,   28,   29,   30,   31,   33,\n",
       "         34,   35,   36,   37,   38,   39,   40,   41,   42,   43,   44,\n",
       "         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n",
       "         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n",
       "         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n",
       "         78,   79,   80,   81,   82,   83,   84,   86,   87,   88,   89,\n",
       "         90,   91,   92,   93,   94,   95,   96,   97,   98,   99,  100,\n",
       "        101,  102,  103,  104,  105,  106,  107,  108,  109,  110,  111,\n",
       "        112,  113,  114,  115,  116,  117,  118,  119,  120,  121,  122,\n",
       "        123,  124,  125,  126,  127,  128,  129,  130,  131,  132,  133,\n",
       "        134,  135,  136,  137,  138,  139,  140,  141,  142,  143,  144,\n",
       "        146,  147,  148,  149,  150,  151,  152,  153,  154,  155,  156,\n",
       "        157,  158,  159,  160,  161,  162,  163,  164,  165,  166,  167,\n",
       "        168,  169,  170,  171,  172,  173,  174,  175,  176,  177,  178,\n",
       "        179,  180,  182,  185,  186,  187,  189,  190,  191,  192,  193,\n",
       "        194,  195,  196,  197,  198,  199,  200,  201,  202,  203,  204,\n",
       "        205,  206,  207,  208,  209,  210,  211,  212,  214,  215,  216,\n",
       "        217,  218,  219,  220,  221,  223,  224,  225,  226,  227,  228,\n",
       "        229,  230,  231,  232,  233,  234,  235,  236,  238,  239,  240,\n",
       "        241,  242,  243,  244,  245,  246,  247,  248,  249,  250,  251,\n",
       "        252,  253,  254,  255,  256,  257,  258,  259,  260,  261,  262,\n",
       "        263,  264,  265,  266,  267,  268,  269,  270,  271,  272,  273,\n",
       "        274,  275,  276,  277,  278,  279,  280,  281,  283,  284,  285,\n",
       "        286,  287,  288,  289,  290,  291,  292,  293,  294,  295,  296,\n",
       "        297,  298,  299,  300,  301,  302,  303,  304,  305,  306,  308,\n",
       "        309,  310,  311,  312,  314,  315,  316,  317,  318,  319,  320,\n",
       "        321,  322,  323,  324,  325,  326,  329,  331,  332,  333,  334,\n",
       "        335,  336,  337,  338,  339,  340,  341,  343,  344,  345,  346,\n",
       "        347,  348,  349,  350,  351,  352,  353,  354,  355,  356,  357,\n",
       "        358,  359,  360,  361,  362,  364,  365,  366,  367,  368,  369,\n",
       "        370,  371,  372,  373,  374,  375,  376,  377,  378,  379,  380,\n",
       "        382,  383,  384,  385,  386,  387,  388,  389,  390,  391,  392,\n",
       "        393,  394,  395,  396,  397,  398,  399,  400,  401,  402,  403,\n",
       "        404,  405,  406,  407,  408,  409,  410,  411,  412,  413,  414,\n",
       "        415,  416,  417,  418,  419,  420,  421,  422,  423,  424,  425,\n",
       "        426,  427,  428,  429,  430,  432,  433,  434,  435,  436,  437,\n",
       "        438,  439,  440,  441,  442,  443,  444,  445,  446,  447,  449,\n",
       "        450,  451,  452,  453,  454,  455,  456,  457,  458,  459,  460,\n",
       "        461,  462,  463,  464,  466,  467,  468,  469,  470,  471,  472,\n",
       "        473,  474,  475,  476,  477,  478,  479,  480,  481,  482,  483,\n",
       "        484,  485,  486,  487,  488,  489,  490,  491,  493,  494,  495,\n",
       "        496,  497,  498,  499,  500,  501,  502,  503,  504,  505,  506,\n",
       "        507,  508,  509,  510,  511,  512,  513,  514,  515,  516,  517,\n",
       "        518,  519,  520,  521,  522,  523,  524,  525,  526,  527,  528,\n",
       "        529,  530,  531,  532,  533,  534,  535,  536,  537,  538,  539,\n",
       "        540,  541,  542,  543,  544,  545,  546,  547,  548,  549,  550,\n",
       "        551,  552,  553,  554,  555,  557,  558,  559,  560,  561,  562,\n",
       "        563,  564,  565,  566,  567,  568,  569,  570,  571,  572,  573,\n",
       "        574,  575,  576,  578,  579,  580,  581,  582,  583,  584,  586,\n",
       "        587,  588,  589,  590,  591,  592,  593,  594,  595,  596,  597,\n",
       "        598,  599,  600,  601,  602,  603,  604,  605,  606,  607,  608,\n",
       "        609,  610,  611,  612,  613,  614,  615,  616,  617,  619,  620,\n",
       "        621,  622,  623,  624,  625,  626,  627,  628,  629,  630,  631,\n",
       "        632,  633,  635,  636,  637,  638,  639,  640,  641,  642,  643,\n",
       "        645,  646,  648,  649,  650,  651,  652,  653,  654,  655,  657,\n",
       "        658,  659,  660,  661,  662,  663,  664,  665,  666,  667,  668,\n",
       "        669,  670,  671,  672,  673,  674,  675,  676,  677,  678,  679,\n",
       "        680,  681,  682,  683,  684,  685,  687,  688,  689,  690,  691,\n",
       "        692,  693,  694,  695,  696,  697,  698,  699,  700,  701,  702,\n",
       "        703,  704,  705,  706,  707,  708,  709,  710,  711,  712,  713,\n",
       "        714,  715,  716,  717,  719,  720,  721,  722,  723,  724,  725,\n",
       "        726,  727,  728,  729,  730,  731,  732,  733,  734,  735,  736,\n",
       "        737,  738,  739,  740,  741,  742,  743,  744,  745,  746,  747,\n",
       "        748,  749,  751,  752,  753,  754,  755,  757,  758,  759,  760,\n",
       "        761,  762,  763,  764,  765,  766,  767,  768,  769,  770,  771,\n",
       "        772,  773,  774,  775,  776,  777,  778,  779,  780,  781,  782,\n",
       "        783,  784,  785,  786,  787,  788,  789,  790,  791,  792,  793,\n",
       "        794,  795,  796,  797,  798,  799,  800,  801,  802,  803,  804,\n",
       "        805,  806,  807,  808,  809,  810,  811,  812,  813,  814,  815,\n",
       "        816,  817,  818,  819,  820,  821,  822,  823,  824,  825,  826,\n",
       "        827,  828,  829,  830,  831,  832,  833,  834,  835,  836,  837,\n",
       "        838,  839,  840,  841,  842,  843,  844,  845,  846,  847,  848,\n",
       "        849,  850,  851,  852,  853,  854,  855,  856,  857,  858,  859,\n",
       "        860,  861,  862,  863,  864,  865,  866,  867,  868,  869,  870,\n",
       "        871,  872,  873,  874,  875,  876,  877,  878,  879,  880,  881,\n",
       "        882,  883,  884,  885,  886,  887,  888,  890,  891,  892,  893,\n",
       "        894,  895,  896,  897,  898,  899,  900,  901,  903,  904,  905,\n",
       "        906,  907,  908,  909,  910,  911,  912,  913,  914,  915,  916,\n",
       "        917,  918,  919,  920,  921,  922,  923,  924,  925,  926,  927,\n",
       "        928,  929,  930,  931,  932,  933,  934,  935,  936,  937,  938,\n",
       "        939,  940,  941,  942,  943,  944,  945,  946,  947,  948,  949,\n",
       "        950,  951,  952,  953,  954,  955,  956,  957,  958,  959,  960,\n",
       "        961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
       "        972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
       "        984,  985,  986,  987,  988,  989,  990,  991,  992,  993,  994,\n",
       "        995,  996,  998,  999, 1000, 1001, 1002, 1003, 1004, 1005, 1007,\n",
       "       1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018,\n",
       "       1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029,\n",
       "       1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(labels)\n",
    "# _sample_mask(test_ids, len(labels)).nonzero()[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = EasyDict({\n",
    "    'graph': G,\n",
    "    'labels': labels,\n",
    "    'onehot_labels': onehot_labels,\n",
    "    'features': feats,\n",
    "    'train_mask':train_mask,\n",
    "    'val_mask': val_mask,\n",
    "    'test_mask': test_mask,\n",
    "    'num_classes': onehot_labels.shape[1],\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def construct_proj_laplacian(laplacian, levels, proj_dir):\n",
    "    coarse_laplacian = []\n",
    "    projections = []\n",
    "    for i in range(levels):\n",
    "        projection_name = \"{}/Projection_{}.mtx\".format(proj_dir, i+1)\n",
    "        projection = mtx2matrix(projection_name)\n",
    "        projections.append(projection)\n",
    "        coarse_laplacian.append(laplacian)\n",
    "        if i != (levels-1):\n",
    "            laplacian = projection @ laplacian @ (projection.transpose())\n",
    "    return projections, coarse_laplacian\n",
    "\n",
    "def mtx2matrix(proj_name):\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    with open(proj_name) as ff:\n",
    "        for i, line in enumerate(ff):\n",
    "            info = line.split()\n",
    "            if i == 0:\n",
    "                NumReducedNodes = int(info[0])\n",
    "                NumOriginNodes = int(info[1])\n",
    "            else:\n",
    "                row.append(int(info[0])-1)\n",
    "                col.append(int(info[1])-1)\n",
    "                data.append(1)\n",
    "    matrix = csr_matrix((data, (row, col)), shape=(\n",
    "        NumReducedNodes, NumOriginNodes))\n",
    "    return matrix\n",
    "levels = 2\n",
    "reduce_results = f\"../graphzoom/reduction_results/Cora\"\n",
    "original_adj = nx.adj_matrix(G)\n",
    "projections, coarse_adj = construct_proj_laplacian(\n",
    "    original_adj, levels, reduce_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1169, 1169)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_adj[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12038585, -0.08481889, -0.14797909, ..., -0.12038585,\n",
       "       -0.14797909, -0.08481889])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_embed = projections[0] * data.features\n",
    "coarse_labels = data.oneho\n",
    "# embed_coarse[0]\n",
    "# data.features[0]\n",
    "# projections[0].shape\n",
    "# data.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # load and preprocess dataset\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = data.labels\n",
    "#     g, features, class_map = load_data(train_prefix)\n",
    "#     labels = torch.LongTensor(list(class_map.values()))\n",
    "    if hasattr(torch, 'BoolTensor'):\n",
    "        train_mask = torch.BoolTensor(data.train_mask)\n",
    "        val_mask = torch.BoolTensor(data.val_mask)\n",
    "        test_mask = torch.BoolTensor(data.test_mask)\n",
    "    in_feats = data.features.shape[1]\n",
    "    n_classes = data.num_classes\n",
    "    n_edges = data.graph.number_of_edges()\n",
    "    print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "              train_mask.int().sum().item(),\n",
    "              val_mask.int().sum().item(),\n",
    "              test_mask.int().sum().item()))\n",
    "\n",
    "    if args.gpu < 0:\n",
    "        cuda = False\n",
    "    else:\n",
    "        cuda = True\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    # graph preprocess and calculate normalization factor\n",
    "    g = data.graph\n",
    "    # add self loop\n",
    "    if args.self_loop:\n",
    "        print('add self_loop')\n",
    "        g.remove_edges_from(nx.selfloop_edges(g))\n",
    "        g.add_edges_from(zip(g.nodes(), g.nodes()))\n",
    "    g = DGLGraph(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "    # normalization\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    if cuda:\n",
    "        norm = norm.cuda()\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    # create GCN model\n",
    "    model = GCN(g,\n",
    "                in_feats,\n",
    "                args.n_hidden,\n",
    "                n_classes,\n",
    "                args.n_layers,\n",
    "                F.relu,\n",
    "                args.dropout)\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=args.lr,\n",
    "                                 weight_decay=args.weight_decay)\n",
    "\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(args.n_epochs):\n",
    "        model.train()\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        # forward\n",
    "        logits = model(features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "        acc = evaluate(model, features, labels, val_mask)\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\n",
    "              \"ETputs(KTEPS) {:.2f}\". format(epoch, np.mean(dur), loss.item(),\n",
    "                                             acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    print()\n",
    "    acc = evaluate(model, features, labels, test_mask)\n",
    "    print(\"Test accuracy {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='cora', dropout=0.5, gpu=0, lr=0.01, n_epochs=200, n_hidden=16, n_layers=1, self_loop=True, weight_decay=0.0005)\n",
      "----Data statistics------'\n",
      "      #Edges 5278\n",
      "      #Classes 7\n",
      "      #Train samples 140\n",
      "      #Val samples 500\n",
      "      #Test samples 1000\n",
      "add self_loop\n",
      "Epoch 00000 | Time(s) nan | Loss 1.9537 | Accuracy 0.5240 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 1.8581 | Accuracy 0.6000 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 1.7498 | Accuracy 0.6380 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.0033 | Loss 1.6401 | Accuracy 0.6600 | ETputs(KTEPS) 4036.95\n",
      "Epoch 00004 | Time(s) 0.0032 | Loss 1.4867 | Accuracy 0.6820 | ETputs(KTEPS) 4108.05\n",
      "Epoch 00005 | Time(s) 0.0032 | Loss 1.3996 | Accuracy 0.7060 | ETputs(KTEPS) 4112.35\n",
      "Epoch 00006 | Time(s) 0.0032 | Loss 1.2872 | Accuracy 0.7320 | ETputs(KTEPS) 4107.37\n",
      "Epoch 00007 | Time(s) 0.0032 | Loss 1.1587 | Accuracy 0.7560 | ETputs(KTEPS) 4108.14\n",
      "Epoch 00008 | Time(s) 0.0032 | Loss 1.0270 | Accuracy 0.7600 | ETputs(KTEPS) 4105.88\n",
      "Epoch 00009 | Time(s) 0.0032 | Loss 0.9550 | Accuracy 0.7700 | ETputs(KTEPS) 4114.67\n",
      "Epoch 00010 | Time(s) 0.0032 | Loss 0.8032 | Accuracy 0.7920 | ETputs(KTEPS) 4110.74\n",
      "Epoch 00011 | Time(s) 0.0032 | Loss 0.7705 | Accuracy 0.8040 | ETputs(KTEPS) 4115.80\n",
      "Epoch 00012 | Time(s) 0.0032 | Loss 0.6529 | Accuracy 0.8160 | ETputs(KTEPS) 4119.27\n",
      "Epoch 00013 | Time(s) 0.0032 | Loss 0.5715 | Accuracy 0.8140 | ETputs(KTEPS) 4125.54\n",
      "Epoch 00014 | Time(s) 0.0032 | Loss 0.5225 | Accuracy 0.8080 | ETputs(KTEPS) 4129.04\n",
      "Epoch 00015 | Time(s) 0.0032 | Loss 0.4535 | Accuracy 0.8120 | ETputs(KTEPS) 4135.00\n",
      "Epoch 00016 | Time(s) 0.0033 | Loss 0.4151 | Accuracy 0.8080 | ETputs(KTEPS) 4080.29\n",
      "Epoch 00017 | Time(s) 0.0032 | Loss 0.3813 | Accuracy 0.8040 | ETputs(KTEPS) 4087.33\n",
      "Epoch 00018 | Time(s) 0.0032 | Loss 0.3563 | Accuracy 0.8040 | ETputs(KTEPS) 4096.72\n",
      "Epoch 00019 | Time(s) 0.0032 | Loss 0.3114 | Accuracy 0.8000 | ETputs(KTEPS) 4101.68\n",
      "Epoch 00020 | Time(s) 0.0032 | Loss 0.3053 | Accuracy 0.7960 | ETputs(KTEPS) 4101.54\n",
      "Epoch 00021 | Time(s) 0.0032 | Loss 0.2774 | Accuracy 0.7980 | ETputs(KTEPS) 4103.00\n",
      "Epoch 00022 | Time(s) 0.0033 | Loss 0.2441 | Accuracy 0.7920 | ETputs(KTEPS) 4028.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v-yulan/.conda/envs/dgl/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/v-yulan/.conda/envs/dgl/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00023 | Time(s) 0.0033 | Loss 0.2125 | Accuracy 0.7900 | ETputs(KTEPS) 4031.52\n",
      "Epoch 00024 | Time(s) 0.0033 | Loss 0.1987 | Accuracy 0.7900 | ETputs(KTEPS) 4014.21\n",
      "Epoch 00025 | Time(s) 0.0033 | Loss 0.1740 | Accuracy 0.7920 | ETputs(KTEPS) 4015.74\n",
      "Epoch 00026 | Time(s) 0.0033 | Loss 0.1655 | Accuracy 0.7920 | ETputs(KTEPS) 4020.49\n",
      "Epoch 00027 | Time(s) 0.0033 | Loss 0.1709 | Accuracy 0.7980 | ETputs(KTEPS) 4021.01\n",
      "Epoch 00028 | Time(s) 0.0033 | Loss 0.1656 | Accuracy 0.7980 | ETputs(KTEPS) 4029.34\n",
      "Epoch 00029 | Time(s) 0.0033 | Loss 0.1754 | Accuracy 0.8000 | ETputs(KTEPS) 4036.90\n",
      "Epoch 00030 | Time(s) 0.0033 | Loss 0.1387 | Accuracy 0.8060 | ETputs(KTEPS) 3991.49\n",
      "Epoch 00031 | Time(s) 0.0033 | Loss 0.1416 | Accuracy 0.8040 | ETputs(KTEPS) 3996.66\n",
      "Epoch 00032 | Time(s) 0.0033 | Loss 0.1151 | Accuracy 0.8100 | ETputs(KTEPS) 4001.66\n",
      "Epoch 00033 | Time(s) 0.0033 | Loss 0.1164 | Accuracy 0.8100 | ETputs(KTEPS) 4005.93\n",
      "Epoch 00034 | Time(s) 0.0033 | Loss 0.1056 | Accuracy 0.8100 | ETputs(KTEPS) 4000.57\n",
      "Epoch 00035 | Time(s) 0.0033 | Loss 0.1030 | Accuracy 0.8120 | ETputs(KTEPS) 3997.89\n",
      "Epoch 00036 | Time(s) 0.0033 | Loss 0.0754 | Accuracy 0.8140 | ETputs(KTEPS) 4000.62\n",
      "Epoch 00037 | Time(s) 0.0033 | Loss 0.0803 | Accuracy 0.8120 | ETputs(KTEPS) 4002.86\n",
      "Epoch 00038 | Time(s) 0.0033 | Loss 0.0974 | Accuracy 0.8060 | ETputs(KTEPS) 4005.67\n",
      "Epoch 00039 | Time(s) 0.0033 | Loss 0.0936 | Accuracy 0.8080 | ETputs(KTEPS) 3996.28\n",
      "Epoch 00040 | Time(s) 0.0033 | Loss 0.0662 | Accuracy 0.8080 | ETputs(KTEPS) 3993.81\n",
      "Epoch 00041 | Time(s) 0.0033 | Loss 0.0717 | Accuracy 0.8100 | ETputs(KTEPS) 3994.47\n",
      "Epoch 00042 | Time(s) 0.0033 | Loss 0.0636 | Accuracy 0.8080 | ETputs(KTEPS) 3997.81\n",
      "Epoch 00043 | Time(s) 0.0033 | Loss 0.0636 | Accuracy 0.8060 | ETputs(KTEPS) 4001.85\n",
      "Epoch 00044 | Time(s) 0.0033 | Loss 0.0631 | Accuracy 0.8060 | ETputs(KTEPS) 4005.60\n",
      "Epoch 00045 | Time(s) 0.0033 | Loss 0.0669 | Accuracy 0.8060 | ETputs(KTEPS) 4011.34\n",
      "Epoch 00046 | Time(s) 0.0033 | Loss 0.0753 | Accuracy 0.8040 | ETputs(KTEPS) 4018.21\n",
      "Epoch 00047 | Time(s) 0.0033 | Loss 0.0722 | Accuracy 0.8020 | ETputs(KTEPS) 4023.65\n",
      "Epoch 00048 | Time(s) 0.0033 | Loss 0.0561 | Accuracy 0.8020 | ETputs(KTEPS) 4027.50\n",
      "Epoch 00049 | Time(s) 0.0033 | Loss 0.0412 | Accuracy 0.8040 | ETputs(KTEPS) 4033.38\n",
      "Epoch 00050 | Time(s) 0.0033 | Loss 0.0733 | Accuracy 0.8040 | ETputs(KTEPS) 4039.24\n",
      "Epoch 00051 | Time(s) 0.0033 | Loss 0.0606 | Accuracy 0.8020 | ETputs(KTEPS) 4044.42\n",
      "Epoch 00052 | Time(s) 0.0033 | Loss 0.0391 | Accuracy 0.8020 | ETputs(KTEPS) 4049.71\n",
      "Epoch 00053 | Time(s) 0.0033 | Loss 0.0664 | Accuracy 0.8040 | ETputs(KTEPS) 4055.09\n",
      "Epoch 00054 | Time(s) 0.0033 | Loss 0.0785 | Accuracy 0.8040 | ETputs(KTEPS) 4059.18\n",
      "Epoch 00055 | Time(s) 0.0033 | Loss 0.0456 | Accuracy 0.8060 | ETputs(KTEPS) 4062.57\n",
      "Epoch 00056 | Time(s) 0.0033 | Loss 0.0424 | Accuracy 0.8060 | ETputs(KTEPS) 4065.20\n",
      "Epoch 00057 | Time(s) 0.0033 | Loss 0.0781 | Accuracy 0.8060 | ETputs(KTEPS) 4067.42\n",
      "Epoch 00058 | Time(s) 0.0033 | Loss 0.0548 | Accuracy 0.8060 | ETputs(KTEPS) 4065.38\n",
      "Epoch 00059 | Time(s) 0.0033 | Loss 0.0505 | Accuracy 0.8100 | ETputs(KTEPS) 4065.14\n",
      "Epoch 00060 | Time(s) 0.0033 | Loss 0.0451 | Accuracy 0.8140 | ETputs(KTEPS) 4066.13\n",
      "Epoch 00061 | Time(s) 0.0033 | Loss 0.0733 | Accuracy 0.8140 | ETputs(KTEPS) 4066.80\n",
      "Epoch 00062 | Time(s) 0.0033 | Loss 0.0402 | Accuracy 0.8160 | ETputs(KTEPS) 4067.66\n",
      "Epoch 00063 | Time(s) 0.0033 | Loss 0.0344 | Accuracy 0.8180 | ETputs(KTEPS) 4062.59\n",
      "Epoch 00064 | Time(s) 0.0033 | Loss 0.0549 | Accuracy 0.8200 | ETputs(KTEPS) 4064.00\n",
      "Epoch 00065 | Time(s) 0.0033 | Loss 0.0502 | Accuracy 0.8220 | ETputs(KTEPS) 4065.88\n",
      "Epoch 00066 | Time(s) 0.0033 | Loss 0.0465 | Accuracy 0.8240 | ETputs(KTEPS) 4066.27\n",
      "Epoch 00067 | Time(s) 0.0033 | Loss 0.0438 | Accuracy 0.8240 | ETputs(KTEPS) 4068.21\n",
      "Epoch 00068 | Time(s) 0.0033 | Loss 0.0583 | Accuracy 0.8240 | ETputs(KTEPS) 4070.13\n",
      "Epoch 00069 | Time(s) 0.0033 | Loss 0.0569 | Accuracy 0.8260 | ETputs(KTEPS) 4071.16\n",
      "Epoch 00070 | Time(s) 0.0033 | Loss 0.0650 | Accuracy 0.8260 | ETputs(KTEPS) 4071.88\n",
      "Epoch 00071 | Time(s) 0.0033 | Loss 0.0497 | Accuracy 0.8200 | ETputs(KTEPS) 4074.32\n",
      "Epoch 00072 | Time(s) 0.0033 | Loss 0.0378 | Accuracy 0.8240 | ETputs(KTEPS) 4076.15\n",
      "Epoch 00073 | Time(s) 0.0033 | Loss 0.0790 | Accuracy 0.8220 | ETputs(KTEPS) 4078.47\n",
      "Epoch 00074 | Time(s) 0.0033 | Loss 0.0452 | Accuracy 0.8200 | ETputs(KTEPS) 4081.08\n",
      "Epoch 00075 | Time(s) 0.0032 | Loss 0.0786 | Accuracy 0.8200 | ETputs(KTEPS) 4083.97\n",
      "Epoch 00076 | Time(s) 0.0032 | Loss 0.0535 | Accuracy 0.8220 | ETputs(KTEPS) 4085.72\n",
      "Epoch 00077 | Time(s) 0.0032 | Loss 0.0514 | Accuracy 0.8200 | ETputs(KTEPS) 4088.12\n",
      "Epoch 00078 | Time(s) 0.0032 | Loss 0.0673 | Accuracy 0.8160 | ETputs(KTEPS) 4090.57\n",
      "Epoch 00079 | Time(s) 0.0032 | Loss 0.0321 | Accuracy 0.8120 | ETputs(KTEPS) 4092.56\n",
      "Epoch 00080 | Time(s) 0.0032 | Loss 0.0491 | Accuracy 0.8100 | ETputs(KTEPS) 4094.75\n",
      "Epoch 00081 | Time(s) 0.0032 | Loss 0.0725 | Accuracy 0.8100 | ETputs(KTEPS) 4096.97\n",
      "Epoch 00082 | Time(s) 0.0032 | Loss 0.0504 | Accuracy 0.8100 | ETputs(KTEPS) 4099.44\n",
      "Epoch 00083 | Time(s) 0.0032 | Loss 0.0500 | Accuracy 0.8140 | ETputs(KTEPS) 4100.85\n",
      "Epoch 00084 | Time(s) 0.0032 | Loss 0.0417 | Accuracy 0.8160 | ETputs(KTEPS) 4093.08\n",
      "Epoch 00085 | Time(s) 0.0032 | Loss 0.0426 | Accuracy 0.8160 | ETputs(KTEPS) 4087.20\n",
      "Epoch 00086 | Time(s) 0.0032 | Loss 0.0482 | Accuracy 0.8180 | ETputs(KTEPS) 4087.65\n",
      "Epoch 00087 | Time(s) 0.0032 | Loss 0.0477 | Accuracy 0.8200 | ETputs(KTEPS) 4087.56\n",
      "Epoch 00088 | Time(s) 0.0032 | Loss 0.0430 | Accuracy 0.8220 | ETputs(KTEPS) 4088.27\n",
      "Epoch 00089 | Time(s) 0.0032 | Loss 0.0447 | Accuracy 0.8200 | ETputs(KTEPS) 4088.77\n",
      "Epoch 00090 | Time(s) 0.0032 | Loss 0.0338 | Accuracy 0.8220 | ETputs(KTEPS) 4091.02\n",
      "Epoch 00091 | Time(s) 0.0032 | Loss 0.0492 | Accuracy 0.8240 | ETputs(KTEPS) 4093.02\n",
      "Epoch 00092 | Time(s) 0.0032 | Loss 0.0347 | Accuracy 0.8240 | ETputs(KTEPS) 4094.78\n",
      "Epoch 00093 | Time(s) 0.0032 | Loss 0.0497 | Accuracy 0.8220 | ETputs(KTEPS) 4096.07\n",
      "Epoch 00094 | Time(s) 0.0032 | Loss 0.0402 | Accuracy 0.8220 | ETputs(KTEPS) 4097.66\n",
      "Epoch 00095 | Time(s) 0.0032 | Loss 0.0475 | Accuracy 0.8220 | ETputs(KTEPS) 4098.98\n",
      "Epoch 00096 | Time(s) 0.0032 | Loss 0.0433 | Accuracy 0.8220 | ETputs(KTEPS) 4100.63\n",
      "Epoch 00097 | Time(s) 0.0032 | Loss 0.0353 | Accuracy 0.8220 | ETputs(KTEPS) 4101.23\n",
      "Epoch 00098 | Time(s) 0.0032 | Loss 0.0338 | Accuracy 0.8220 | ETputs(KTEPS) 4100.96\n",
      "Epoch 00099 | Time(s) 0.0032 | Loss 0.0503 | Accuracy 0.8240 | ETputs(KTEPS) 4100.99\n",
      "Epoch 00100 | Time(s) 0.0032 | Loss 0.0382 | Accuracy 0.8240 | ETputs(KTEPS) 4103.01\n",
      "Epoch 00101 | Time(s) 0.0032 | Loss 0.0438 | Accuracy 0.8260 | ETputs(KTEPS) 4103.06\n",
      "Epoch 00102 | Time(s) 0.0032 | Loss 0.0302 | Accuracy 0.8240 | ETputs(KTEPS) 4103.68\n",
      "Epoch 00103 | Time(s) 0.0032 | Loss 0.0380 | Accuracy 0.8260 | ETputs(KTEPS) 4104.28\n",
      "Epoch 00104 | Time(s) 0.0032 | Loss 0.0642 | Accuracy 0.8200 | ETputs(KTEPS) 4103.87\n",
      "Epoch 00105 | Time(s) 0.0032 | Loss 0.0293 | Accuracy 0.8200 | ETputs(KTEPS) 4104.43\n",
      "Epoch 00106 | Time(s) 0.0032 | Loss 0.0576 | Accuracy 0.8200 | ETputs(KTEPS) 4105.03\n",
      "Epoch 00107 | Time(s) 0.0032 | Loss 0.0596 | Accuracy 0.8200 | ETputs(KTEPS) 4106.12\n",
      "Epoch 00108 | Time(s) 0.0032 | Loss 0.0536 | Accuracy 0.8200 | ETputs(KTEPS) 4107.19\n",
      "Epoch 00109 | Time(s) 0.0032 | Loss 0.0489 | Accuracy 0.8220 | ETputs(KTEPS) 4108.53\n",
      "Epoch 00110 | Time(s) 0.0032 | Loss 0.0447 | Accuracy 0.8200 | ETputs(KTEPS) 4109.60\n",
      "Epoch 00111 | Time(s) 0.0032 | Loss 0.0413 | Accuracy 0.8180 | ETputs(KTEPS) 4110.63\n",
      "Epoch 00112 | Time(s) 0.0032 | Loss 0.0334 | Accuracy 0.8180 | ETputs(KTEPS) 4110.69\n",
      "Epoch 00113 | Time(s) 0.0032 | Loss 0.0457 | Accuracy 0.8180 | ETputs(KTEPS) 4111.07\n",
      "Epoch 00114 | Time(s) 0.0032 | Loss 0.0447 | Accuracy 0.8180 | ETputs(KTEPS) 4111.80\n",
      "Epoch 00115 | Time(s) 0.0032 | Loss 0.0265 | Accuracy 0.8180 | ETputs(KTEPS) 4111.85\n",
      "Epoch 00116 | Time(s) 0.0032 | Loss 0.0286 | Accuracy 0.8180 | ETputs(KTEPS) 4112.04\n",
      "Epoch 00117 | Time(s) 0.0032 | Loss 0.0318 | Accuracy 0.8180 | ETputs(KTEPS) 4111.84\n",
      "Epoch 00118 | Time(s) 0.0032 | Loss 0.0241 | Accuracy 0.8180 | ETputs(KTEPS) 4112.24\n",
      "Epoch 00119 | Time(s) 0.0032 | Loss 0.0302 | Accuracy 0.8180 | ETputs(KTEPS) 4110.59\n",
      "Epoch 00120 | Time(s) 0.0032 | Loss 0.0275 | Accuracy 0.8180 | ETputs(KTEPS) 4107.97\n",
      "Epoch 00121 | Time(s) 0.0032 | Loss 0.0435 | Accuracy 0.8180 | ETputs(KTEPS) 4108.03\n",
      "Epoch 00122 | Time(s) 0.0032 | Loss 0.0535 | Accuracy 0.8200 | ETputs(KTEPS) 4107.71\n",
      "Epoch 00123 | Time(s) 0.0032 | Loss 0.0459 | Accuracy 0.8200 | ETputs(KTEPS) 4109.42\n",
      "Epoch 00124 | Time(s) 0.0032 | Loss 0.0417 | Accuracy 0.8200 | ETputs(KTEPS) 4110.08\n",
      "Epoch 00125 | Time(s) 0.0032 | Loss 0.0291 | Accuracy 0.8220 | ETputs(KTEPS) 4110.59\n",
      "Epoch 00126 | Time(s) 0.0032 | Loss 0.0509 | Accuracy 0.8220 | ETputs(KTEPS) 4111.13\n",
      "Epoch 00127 | Time(s) 0.0032 | Loss 0.0522 | Accuracy 0.8220 | ETputs(KTEPS) 4111.98\n",
      "Epoch 00128 | Time(s) 0.0032 | Loss 0.0525 | Accuracy 0.8220 | ETputs(KTEPS) 4112.43\n",
      "Epoch 00129 | Time(s) 0.0032 | Loss 0.0672 | Accuracy 0.8240 | ETputs(KTEPS) 4112.86\n",
      "Epoch 00130 | Time(s) 0.0032 | Loss 0.0321 | Accuracy 0.8240 | ETputs(KTEPS) 4113.38\n",
      "Epoch 00131 | Time(s) 0.0032 | Loss 0.0324 | Accuracy 0.8260 | ETputs(KTEPS) 4114.34\n",
      "Epoch 00132 | Time(s) 0.0032 | Loss 0.0463 | Accuracy 0.8320 | ETputs(KTEPS) 4115.10\n",
      "Epoch 00133 | Time(s) 0.0032 | Loss 0.0333 | Accuracy 0.8320 | ETputs(KTEPS) 4115.30\n",
      "Epoch 00134 | Time(s) 0.0032 | Loss 0.0362 | Accuracy 0.8300 | ETputs(KTEPS) 4115.79\n",
      "Epoch 00135 | Time(s) 0.0032 | Loss 0.0323 | Accuracy 0.8300 | ETputs(KTEPS) 4116.28\n",
      "Epoch 00136 | Time(s) 0.0032 | Loss 0.0343 | Accuracy 0.8320 | ETputs(KTEPS) 4116.01\n",
      "Epoch 00137 | Time(s) 0.0032 | Loss 0.0351 | Accuracy 0.8320 | ETputs(KTEPS) 4116.30\n",
      "Epoch 00138 | Time(s) 0.0032 | Loss 0.0442 | Accuracy 0.8320 | ETputs(KTEPS) 4116.83\n",
      "Epoch 00139 | Time(s) 0.0032 | Loss 0.0362 | Accuracy 0.8320 | ETputs(KTEPS) 4117.68\n",
      "Epoch 00140 | Time(s) 0.0032 | Loss 0.0406 | Accuracy 0.8300 | ETputs(KTEPS) 4118.00\n",
      "Epoch 00141 | Time(s) 0.0032 | Loss 0.0347 | Accuracy 0.8320 | ETputs(KTEPS) 4118.49\n",
      "Epoch 00142 | Time(s) 0.0032 | Loss 0.0323 | Accuracy 0.8340 | ETputs(KTEPS) 4119.14\n",
      "Epoch 00143 | Time(s) 0.0032 | Loss 0.0434 | Accuracy 0.8320 | ETputs(KTEPS) 4115.82\n",
      "Epoch 00144 | Time(s) 0.0032 | Loss 0.0426 | Accuracy 0.8320 | ETputs(KTEPS) 4115.36\n",
      "Epoch 00145 | Time(s) 0.0032 | Loss 0.0511 | Accuracy 0.8320 | ETputs(KTEPS) 4115.29\n",
      "Epoch 00146 | Time(s) 0.0032 | Loss 0.0374 | Accuracy 0.8340 | ETputs(KTEPS) 4115.95\n",
      "Epoch 00147 | Time(s) 0.0032 | Loss 0.0323 | Accuracy 0.8320 | ETputs(KTEPS) 4115.16\n",
      "Epoch 00148 | Time(s) 0.0032 | Loss 0.0396 | Accuracy 0.8300 | ETputs(KTEPS) 4115.69\n",
      "Epoch 00149 | Time(s) 0.0032 | Loss 0.0398 | Accuracy 0.8260 | ETputs(KTEPS) 4116.19\n",
      "Epoch 00150 | Time(s) 0.0032 | Loss 0.0370 | Accuracy 0.8260 | ETputs(KTEPS) 4117.21\n",
      "Epoch 00151 | Time(s) 0.0032 | Loss 0.0339 | Accuracy 0.8320 | ETputs(KTEPS) 4117.09\n",
      "Epoch 00152 | Time(s) 0.0032 | Loss 0.0364 | Accuracy 0.8280 | ETputs(KTEPS) 4117.25\n",
      "Epoch 00153 | Time(s) 0.0032 | Loss 0.0436 | Accuracy 0.8280 | ETputs(KTEPS) 4118.31\n",
      "Epoch 00154 | Time(s) 0.0032 | Loss 0.0366 | Accuracy 0.8280 | ETputs(KTEPS) 4118.93\n",
      "Epoch 00155 | Time(s) 0.0032 | Loss 0.0501 | Accuracy 0.8280 | ETputs(KTEPS) 4118.97\n",
      "Epoch 00156 | Time(s) 0.0032 | Loss 0.0493 | Accuracy 0.8280 | ETputs(KTEPS) 4118.20\n",
      "Epoch 00157 | Time(s) 0.0032 | Loss 0.0347 | Accuracy 0.8320 | ETputs(KTEPS) 4118.76\n",
      "Epoch 00158 | Time(s) 0.0032 | Loss 0.0282 | Accuracy 0.8320 | ETputs(KTEPS) 4118.74\n",
      "Epoch 00159 | Time(s) 0.0032 | Loss 0.0571 | Accuracy 0.8300 | ETputs(KTEPS) 4119.14\n",
      "Epoch 00160 | Time(s) 0.0032 | Loss 0.0415 | Accuracy 0.8340 | ETputs(KTEPS) 4118.76\n",
      "Epoch 00161 | Time(s) 0.0032 | Loss 0.0502 | Accuracy 0.8340 | ETputs(KTEPS) 4118.56\n",
      "Epoch 00162 | Time(s) 0.0032 | Loss 0.0321 | Accuracy 0.8380 | ETputs(KTEPS) 4119.01\n",
      "Epoch 00163 | Time(s) 0.0032 | Loss 0.0269 | Accuracy 0.8380 | ETputs(KTEPS) 4119.25\n",
      "Epoch 00164 | Time(s) 0.0032 | Loss 0.0292 | Accuracy 0.8360 | ETputs(KTEPS) 4119.94\n",
      "Epoch 00165 | Time(s) 0.0032 | Loss 0.0237 | Accuracy 0.8320 | ETputs(KTEPS) 4120.12\n",
      "Epoch 00166 | Time(s) 0.0032 | Loss 0.0368 | Accuracy 0.8320 | ETputs(KTEPS) 4119.59\n",
      "Epoch 00167 | Time(s) 0.0032 | Loss 0.0353 | Accuracy 0.8320 | ETputs(KTEPS) 4119.39\n",
      "Epoch 00168 | Time(s) 0.0032 | Loss 0.0315 | Accuracy 0.8300 | ETputs(KTEPS) 4120.94\n",
      "Epoch 00169 | Time(s) 0.0032 | Loss 0.0220 | Accuracy 0.8300 | ETputs(KTEPS) 4120.54\n",
      "Epoch 00170 | Time(s) 0.0032 | Loss 0.0342 | Accuracy 0.8300 | ETputs(KTEPS) 4120.54\n",
      "Epoch 00171 | Time(s) 0.0032 | Loss 0.0306 | Accuracy 0.8280 | ETputs(KTEPS) 4120.33\n",
      "Epoch 00172 | Time(s) 0.0032 | Loss 0.0206 | Accuracy 0.8280 | ETputs(KTEPS) 4120.33\n",
      "Epoch 00173 | Time(s) 0.0032 | Loss 0.0413 | Accuracy 0.8280 | ETputs(KTEPS) 4120.22\n",
      "Epoch 00174 | Time(s) 0.0032 | Loss 0.0292 | Accuracy 0.8280 | ETputs(KTEPS) 4120.21\n",
      "Epoch 00175 | Time(s) 0.0032 | Loss 0.0349 | Accuracy 0.8300 | ETputs(KTEPS) 4120.44\n",
      "Epoch 00176 | Time(s) 0.0032 | Loss 0.0306 | Accuracy 0.8300 | ETputs(KTEPS) 4120.44\n",
      "Epoch 00177 | Time(s) 0.0032 | Loss 0.0257 | Accuracy 0.8300 | ETputs(KTEPS) 4120.39\n",
      "Epoch 00178 | Time(s) 0.0032 | Loss 0.0289 | Accuracy 0.8320 | ETputs(KTEPS) 4120.61\n",
      "Epoch 00179 | Time(s) 0.0032 | Loss 0.0362 | Accuracy 0.8340 | ETputs(KTEPS) 4120.79\n",
      "Epoch 00180 | Time(s) 0.0032 | Loss 0.0357 | Accuracy 0.8360 | ETputs(KTEPS) 4120.72\n",
      "Epoch 00181 | Time(s) 0.0032 | Loss 0.0372 | Accuracy 0.8340 | ETputs(KTEPS) 4120.90\n",
      "Epoch 00182 | Time(s) 0.0032 | Loss 0.0275 | Accuracy 0.8340 | ETputs(KTEPS) 4121.20\n",
      "Epoch 00183 | Time(s) 0.0032 | Loss 0.0295 | Accuracy 0.8320 | ETputs(KTEPS) 4119.73\n",
      "Epoch 00184 | Time(s) 0.0032 | Loss 0.0215 | Accuracy 0.8300 | ETputs(KTEPS) 4119.34\n",
      "Epoch 00185 | Time(s) 0.0032 | Loss 0.0497 | Accuracy 0.8280 | ETputs(KTEPS) 4119.80\n",
      "Epoch 00186 | Time(s) 0.0032 | Loss 0.0244 | Accuracy 0.8280 | ETputs(KTEPS) 4120.49\n",
      "Epoch 00187 | Time(s) 0.0032 | Loss 0.0449 | Accuracy 0.8280 | ETputs(KTEPS) 4120.90\n",
      "Epoch 00188 | Time(s) 0.0032 | Loss 0.0374 | Accuracy 0.8280 | ETputs(KTEPS) 4121.21\n",
      "Epoch 00189 | Time(s) 0.0032 | Loss 0.0357 | Accuracy 0.8260 | ETputs(KTEPS) 4121.12\n",
      "Epoch 00190 | Time(s) 0.0032 | Loss 0.0232 | Accuracy 0.8260 | ETputs(KTEPS) 4121.30\n",
      "Epoch 00191 | Time(s) 0.0032 | Loss 0.0301 | Accuracy 0.8260 | ETputs(KTEPS) 4120.93\n",
      "Epoch 00192 | Time(s) 0.0032 | Loss 0.0286 | Accuracy 0.8260 | ETputs(KTEPS) 4120.82\n",
      "Epoch 00193 | Time(s) 0.0032 | Loss 0.0297 | Accuracy 0.8240 | ETputs(KTEPS) 4119.02\n",
      "Epoch 00194 | Time(s) 0.0032 | Loss 0.0350 | Accuracy 0.8200 | ETputs(KTEPS) 4119.25\n",
      "Epoch 00195 | Time(s) 0.0032 | Loss 0.0296 | Accuracy 0.8200 | ETputs(KTEPS) 4119.13\n",
      "Epoch 00196 | Time(s) 0.0032 | Loss 0.0315 | Accuracy 0.8220 | ETputs(KTEPS) 4119.69\n",
      "Epoch 00197 | Time(s) 0.0032 | Loss 0.0308 | Accuracy 0.8220 | ETputs(KTEPS) 4119.78\n",
      "Epoch 00198 | Time(s) 0.0032 | Loss 0.0838 | Accuracy 0.8200 | ETputs(KTEPS) 4119.85\n",
      "Epoch 00199 | Time(s) 0.0032 | Loss 0.0378 | Accuracy 0.8220 | ETputs(KTEPS) 4119.59\n",
      "\n",
      "Test accuracy 81.10%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='GCN')\n",
    "#     register_data_args(parser)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.5,\n",
    "            help=\"dropout probability\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "            help=\"gpu\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-2,\n",
    "            help=\"learning rate\")\n",
    "    parser.add_argument(\"--n-epochs\", type=int, default=200,\n",
    "            help=\"number of training epochs\")\n",
    "    parser.add_argument(\"--n-hidden\", type=int, default=16,\n",
    "            help=\"number of hidden gcn units\")\n",
    "    parser.add_argument(\"--n-layers\", type=int, default=1,\n",
    "            help=\"number of hidden gcn layers\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=5e-4,\n",
    "            help=\"Weight for L2 loss\")\n",
    "    parser.add_argument(\"--self-loop\", action='store_true',\n",
    "            help=\"graph self-loop (default=False)\")\n",
    "#     parser.add_argument(\"--dataset\", default='cora')\n",
    "    parser.set_defaults(self_loop=True)\n",
    "    parser.add_argument(\"--dataset\", default='cora')\n",
    "#     args = parser.parse_args()\n",
    "#     args = parser.parse_args()[1:]\n",
    "    args = parser.parse_known_args()[0]\n",
    "    print(args)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments\n",
    "dataset | level | acc\n",
    "--- | --- | ---\n",
    "cora | 0 | 81.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
